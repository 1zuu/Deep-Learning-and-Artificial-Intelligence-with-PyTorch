{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cbc0545-bf9e-439a-95b4-34cd83d23338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5ba27f-3df1-474a-b36b-a4fca46791bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fe4ac3-3ad1-4226-ba50-744ad55ce37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  Shape : (569, 30)\n",
      "Output Shape : (569,)\n"
     ]
    }
   ],
   "source": [
    "X = data['data']\n",
    "Y = data['target']\n",
    "\n",
    "print(\"Input  Shape : {}\".format(X.shape))\n",
    "print(\"Output Shape : {}\".format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15c77bd-ecd4-46c8-bf35-cc5fa40428ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Input  Shape : (427, 30)\n",
      "Train Output Shape : (427,)\n",
      "\n",
      "Test  Input  Shape : (142, 30)\n",
      "Test  Output Shape : (142,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.25\n",
    "Ntest = int(len(X) * test_size)\n",
    "\n",
    "test_idxs = np.random.choice(len(X), Ntest, replace=False)\n",
    "\n",
    "mask = np.ones(Y.size, dtype=bool)\n",
    "mask[test_idxs] = False\n",
    "\n",
    "Xtrain = X[mask]\n",
    "Ytrain = Y[mask]\n",
    "\n",
    "Xtest = X[test_idxs]\n",
    "Ytest = Y[test_idxs]\n",
    "\n",
    "print(\"Train Input  Shape : {}\".format(Xtrain.shape))\n",
    "print(\"Train Output Shape : {}\\n\".format(Ytrain.shape))\n",
    "\n",
    "print(\"Test  Input  Shape : {}\".format(Xtest.shape))\n",
    "print(\"Test  Output Shape : {}\\n\".format(Ytest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ea856b-58dc-491a-851c-15e93dc810b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "scalar.fit(Xtrain)\n",
    "\n",
    "Xtrain = scalar.transform(Xtrain)\n",
    "Xtest  = scalar.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b0d31b-86e5-41c7-8adb-4730a0e29247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "                nn.Linear(Xtrain.shape[1], 1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e6adec-4d66-407d-b782-1fa1d43ed6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Please Make sure to convert your targets into 2D array such like (N,1) shape. This isn't needed\n",
    "in other frameworks such as tensorflow, keras & scikit-learn. But in pytroch you make sure to\n",
    "unsqueeze targets\n",
    "'''\n",
    "Xtrain = torch.from_numpy(Xtrain.astype(np.float32))\n",
    "Ytrain = torch.from_numpy(Ytrain.astype(np.float32).reshape(-1,1))\n",
    "\n",
    "Xtest = torch.from_numpy(Xtest.astype(np.float32))\n",
    "Ytest = torch.from_numpy(Ytest.astype(np.float32).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a47dcb7-8955-4498-b841-1ed6a8299d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000 , Train Loss : 0.7112409472465515 , Test Loss : 0.6951330900192261\n",
      "epoch 2/1000 , Train Loss : 0.7043485641479492 , Test Loss : 0.6886696219444275\n",
      "epoch 3/1000 , Train Loss : 0.6975345611572266 , Test Loss : 0.6822892427444458\n",
      "epoch 4/1000 , Train Loss : 0.6908000707626343 , Test Loss : 0.6759865283966064\n",
      "epoch 5/1000 , Train Loss : 0.6841455101966858 , Test Loss : 0.6697537302970886\n",
      "epoch 6/1000 , Train Loss : 0.6775710582733154 , Test Loss : 0.663583517074585\n",
      "epoch 7/1000 , Train Loss : 0.6710764765739441 , Test Loss : 0.6574742197990417\n",
      "epoch 8/1000 , Train Loss : 0.6646612286567688 , Test Loss : 0.65142822265625\n",
      "epoch 9/1000 , Train Loss : 0.6583252549171448 , Test Loss : 0.6454495191574097\n",
      "epoch 10/1000 , Train Loss : 0.6520687341690063 , Test Loss : 0.6395416855812073\n",
      "epoch 11/1000 , Train Loss : 0.6458920240402222 , Test Loss : 0.633707582950592\n",
      "epoch 12/1000 , Train Loss : 0.6397956013679504 , Test Loss : 0.6279495358467102\n",
      "epoch 13/1000 , Train Loss : 0.6337793469429016 , Test Loss : 0.6222690939903259\n",
      "epoch 14/1000 , Train Loss : 0.6278437376022339 , Test Loss : 0.6166676878929138\n",
      "epoch 15/1000 , Train Loss : 0.6219888925552368 , Test Loss : 0.611146092414856\n",
      "epoch 16/1000 , Train Loss : 0.6162148118019104 , Test Loss : 0.6057052612304688\n",
      "epoch 17/1000 , Train Loss : 0.6105213761329651 , Test Loss : 0.6003456711769104\n",
      "epoch 18/1000 , Train Loss : 0.6049085855484009 , Test Loss : 0.5950676798820496\n",
      "epoch 19/1000 , Train Loss : 0.5993760824203491 , Test Loss : 0.5898715257644653\n",
      "epoch 20/1000 , Train Loss : 0.593923807144165 , Test Loss : 0.5847572684288025\n",
      "epoch 21/1000 , Train Loss : 0.5885511040687561 , Test Loss : 0.5797245502471924\n",
      "epoch 22/1000 , Train Loss : 0.583257794380188 , Test Loss : 0.5747727751731873\n",
      "epoch 23/1000 , Train Loss : 0.5780432224273682 , Test Loss : 0.5699012875556946\n",
      "epoch 24/1000 , Train Loss : 0.5729067921638489 , Test Loss : 0.5651089549064636\n",
      "epoch 25/1000 , Train Loss : 0.5678480863571167 , Test Loss : 0.5603946447372437\n",
      "epoch 26/1000 , Train Loss : 0.5628662705421448 , Test Loss : 0.5557569861412048\n",
      "epoch 27/1000 , Train Loss : 0.5579606890678406 , Test Loss : 0.5511945486068726\n",
      "epoch 28/1000 , Train Loss : 0.5531306266784668 , Test Loss : 0.5467056632041931\n",
      "epoch 29/1000 , Train Loss : 0.5483752489089966 , Test Loss : 0.5422889590263367\n",
      "epoch 30/1000 , Train Loss : 0.5436936020851135 , Test Loss : 0.5379427671432495\n",
      "epoch 31/1000 , Train Loss : 0.5390849709510803 , Test Loss : 0.5336654186248779\n",
      "epoch 32/1000 , Train Loss : 0.534548282623291 , Test Loss : 0.5294554233551025\n",
      "epoch 33/1000 , Train Loss : 0.5300827622413635 , Test Loss : 0.525311291217804\n",
      "epoch 34/1000 , Train Loss : 0.5256872177124023 , Test Loss : 0.5212312340736389\n",
      "epoch 35/1000 , Train Loss : 0.5213607549667358 , Test Loss : 0.5172141790390015\n",
      "epoch 36/1000 , Train Loss : 0.5171023607254028 , Test Loss : 0.5132585167884827\n",
      "epoch 37/1000 , Train Loss : 0.5129110217094421 , Test Loss : 0.5093631744384766\n",
      "epoch 38/1000 , Train Loss : 0.5087854862213135 , Test Loss : 0.5055270195007324\n",
      "epoch 39/1000 , Train Loss : 0.50472491979599 , Test Loss : 0.5017488598823547\n",
      "epoch 40/1000 , Train Loss : 0.5007280707359314 , Test Loss : 0.4980277419090271\n",
      "epoch 41/1000 , Train Loss : 0.4967939853668213 , Test Loss : 0.4943626821041107\n",
      "epoch 42/1000 , Train Loss : 0.49292153120040894 , Test Loss : 0.4907529652118683\n",
      "epoch 43/1000 , Train Loss : 0.4891095757484436 , Test Loss : 0.48719748854637146\n",
      "epoch 44/1000 , Train Loss : 0.4853571653366089 , Test Loss : 0.4836955964565277\n",
      "epoch 45/1000 , Train Loss : 0.48166313767433167 , Test Loss : 0.48024627566337585\n",
      "epoch 46/1000 , Train Loss : 0.478026419878006 , Test Loss : 0.47684866189956665\n",
      "epoch 47/1000 , Train Loss : 0.4744459092617035 , Test Loss : 0.47350189089775085\n",
      "epoch 48/1000 , Train Loss : 0.4709206819534302 , Test Loss : 0.4702049493789673\n",
      "epoch 49/1000 , Train Loss : 0.46744948625564575 , Test Loss : 0.4669569432735443\n",
      "epoch 50/1000 , Train Loss : 0.4640314280986786 , Test Loss : 0.4637567698955536\n",
      "epoch 51/1000 , Train Loss : 0.4606655240058899 , Test Loss : 0.4606035351753235\n",
      "epoch 52/1000 , Train Loss : 0.4573507308959961 , Test Loss : 0.4574959874153137\n",
      "epoch 53/1000 , Train Loss : 0.45408591628074646 , Test Loss : 0.45443329215049744\n",
      "epoch 54/1000 , Train Loss : 0.4508703052997589 , Test Loss : 0.4514142572879791\n",
      "epoch 55/1000 , Train Loss : 0.4477027952671051 , Test Loss : 0.44843778014183044\n",
      "epoch 56/1000 , Train Loss : 0.4445825219154358 , Test Loss : 0.44550299644470215\n",
      "epoch 57/1000 , Train Loss : 0.4415085017681122 , Test Loss : 0.44260886311531067\n",
      "epoch 58/1000 , Train Loss : 0.43847984075546265 , Test Loss : 0.4397543668746948\n",
      "epoch 59/1000 , Train Loss : 0.4354957044124603 , Test Loss : 0.43693870306015015\n",
      "epoch 60/1000 , Train Loss : 0.4325551986694336 , Test Loss : 0.43416088819503784\n",
      "epoch 61/1000 , Train Loss : 0.42965736985206604 , Test Loss : 0.43142014741897583\n",
      "epoch 62/1000 , Train Loss : 0.42680150270462036 , Test Loss : 0.4287157654762268\n",
      "epoch 63/1000 , Train Loss : 0.4239867031574249 , Test Loss : 0.42604705691337585\n",
      "epoch 64/1000 , Train Loss : 0.42121219635009766 , Test Loss : 0.4234131872653961\n",
      "epoch 65/1000 , Train Loss : 0.4184771478176117 , Test Loss : 0.42081359028816223\n",
      "epoch 66/1000 , Train Loss : 0.41578084230422974 , Test Loss : 0.41824784874916077\n",
      "epoch 67/1000 , Train Loss : 0.4131225347518921 , Test Loss : 0.41571515798568726\n",
      "epoch 68/1000 , Train Loss : 0.4105013906955719 , Test Loss : 0.41321513056755066\n",
      "epoch 69/1000 , Train Loss : 0.4079168140888214 , Test Loss : 0.41074714064598083\n",
      "epoch 70/1000 , Train Loss : 0.40536800026893616 , Test Loss : 0.40831074118614197\n",
      "epoch 71/1000 , Train Loss : 0.4028543531894684 , Test Loss : 0.40590545535087585\n",
      "epoch 72/1000 , Train Loss : 0.4003751277923584 , Test Loss : 0.40353071689605713\n",
      "epoch 73/1000 , Train Loss : 0.39792969822883606 , Test Loss : 0.4011860489845276\n",
      "epoch 74/1000 , Train Loss : 0.3955174684524536 , Test Loss : 0.39887094497680664\n",
      "epoch 75/1000 , Train Loss : 0.39313772320747375 , Test Loss : 0.39658501744270325\n",
      "epoch 76/1000 , Train Loss : 0.39078986644744873 , Test Loss : 0.3943275213241577\n",
      "epoch 77/1000 , Train Loss : 0.38847336173057556 , Test Loss : 0.39209818840026855\n",
      "epoch 78/1000 , Train Loss : 0.3861875832080841 , Test Loss : 0.3898963928222656\n",
      "epoch 79/1000 , Train Loss : 0.3839319050312042 , Test Loss : 0.38772159814834595\n",
      "epoch 80/1000 , Train Loss : 0.3817059099674225 , Test Loss : 0.38557347655296326\n",
      "epoch 81/1000 , Train Loss : 0.3795088827610016 , Test Loss : 0.38345131278038025\n",
      "epoch 82/1000 , Train Loss : 0.3773403763771057 , Test Loss : 0.38135483860969543\n",
      "epoch 83/1000 , Train Loss : 0.3751998543739319 , Test Loss : 0.37928348779678345\n",
      "epoch 84/1000 , Train Loss : 0.3730868399143219 , Test Loss : 0.37723681330680847\n",
      "epoch 85/1000 , Train Loss : 0.3710007667541504 , Test Loss : 0.3752143383026123\n",
      "epoch 86/1000 , Train Loss : 0.36894121766090393 , Test Loss : 0.3732157349586487\n",
      "epoch 87/1000 , Train Loss : 0.36690768599510193 , Test Loss : 0.3712405264377594\n",
      "epoch 88/1000 , Train Loss : 0.3648996949195862 , Test Loss : 0.3692883551120758\n",
      "epoch 89/1000 , Train Loss : 0.36291682720184326 , Test Loss : 0.3673588037490845\n",
      "epoch 90/1000 , Train Loss : 0.3609585165977478 , Test Loss : 0.36545154452323914\n",
      "epoch 91/1000 , Train Loss : 0.3590244650840759 , Test Loss : 0.36356616020202637\n",
      "epoch 92/1000 , Train Loss : 0.35711419582366943 , Test Loss : 0.3617023825645447\n",
      "epoch 93/1000 , Train Loss : 0.3552272319793701 , Test Loss : 0.3598599135875702\n",
      "epoch 94/1000 , Train Loss : 0.3533632457256317 , Test Loss : 0.35803836584091187\n",
      "epoch 95/1000 , Train Loss : 0.3515218198299408 , Test Loss : 0.3562374413013458\n",
      "epoch 96/1000 , Train Loss : 0.3497025668621063 , Test Loss : 0.3544568121433258\n",
      "epoch 97/1000 , Train Loss : 0.34790509939193726 , Test Loss : 0.3526962101459503\n",
      "epoch 98/1000 , Train Loss : 0.34612900018692017 , Test Loss : 0.3509552776813507\n",
      "epoch 99/1000 , Train Loss : 0.3443739712238312 , Test Loss : 0.34923380613327026\n",
      "epoch 100/1000 , Train Loss : 0.3426395654678345 , Test Loss : 0.34753143787384033\n",
      "epoch 101/1000 , Train Loss : 0.34092554450035095 , Test Loss : 0.34584781527519226\n",
      "epoch 102/1000 , Train Loss : 0.3392314612865448 , Test Loss : 0.3441827595233917\n",
      "epoch 103/1000 , Train Loss : 0.3375570476055145 , Test Loss : 0.34253597259521484\n",
      "epoch 104/1000 , Train Loss : 0.3359019458293915 , Test Loss : 0.34090709686279297\n",
      "epoch 105/1000 , Train Loss : 0.334265798330307 , Test Loss : 0.3392958343029022\n",
      "epoch 106/1000 , Train Loss : 0.332648366689682 , Test Loss : 0.3377020061016083\n",
      "epoch 107/1000 , Train Loss : 0.33104926347732544 , Test Loss : 0.3361252248287201\n",
      "epoch 108/1000 , Train Loss : 0.3294682502746582 , Test Loss : 0.33456534147262573\n",
      "epoch 109/1000 , Train Loss : 0.32790496945381165 , Test Loss : 0.33302199840545654\n",
      "epoch 110/1000 , Train Loss : 0.32635918259620667 , Test Loss : 0.33149489760398865\n",
      "epoch 111/1000 , Train Loss : 0.3248305916786194 , Test Loss : 0.3299839496612549\n",
      "epoch 112/1000 , Train Loss : 0.3233188986778259 , Test Loss : 0.3284887671470642\n",
      "epoch 113/1000 , Train Loss : 0.3218238353729248 , Test Loss : 0.3270091116428375\n",
      "epoch 114/1000 , Train Loss : 0.3203451633453369 , Test Loss : 0.3255448043346405\n",
      "epoch 115/1000 , Train Loss : 0.31888261437416077 , Test Loss : 0.32409563660621643\n",
      "epoch 116/1000 , Train Loss : 0.3174358904361725 , Test Loss : 0.3226613402366638\n",
      "epoch 117/1000 , Train Loss : 0.3160047233104706 , Test Loss : 0.32124173641204834\n",
      "epoch 118/1000 , Train Loss : 0.31458890438079834 , Test Loss : 0.3198365569114685\n",
      "epoch 119/1000 , Train Loss : 0.31318825483322144 , Test Loss : 0.3184455931186676\n",
      "epoch 120/1000 , Train Loss : 0.3118024170398712 , Test Loss : 0.3170686960220337\n",
      "epoch 121/1000 , Train Loss : 0.31043124198913574 , Test Loss : 0.31570565700531006\n",
      "epoch 122/1000 , Train Loss : 0.3090744614601135 , Test Loss : 0.31435626745224\n",
      "epoch 123/1000 , Train Loss : 0.30773189663887024 , Test Loss : 0.313020259141922\n",
      "epoch 124/1000 , Train Loss : 0.306403249502182 , Test Loss : 0.3116975426673889\n",
      "epoch 125/1000 , Train Loss : 0.3050883710384369 , Test Loss : 0.31038787961006165\n",
      "epoch 126/1000 , Train Loss : 0.3037870228290558 , Test Loss : 0.30909112095832825\n",
      "epoch 127/1000 , Train Loss : 0.30249902606010437 , Test Loss : 0.3078070878982544\n",
      "epoch 128/1000 , Train Loss : 0.3012241721153259 , Test Loss : 0.306535542011261\n",
      "epoch 129/1000 , Train Loss : 0.29996219277381897 , Test Loss : 0.3052763342857361\n",
      "epoch 130/1000 , Train Loss : 0.29871299862861633 , Test Loss : 0.30402931571006775\n",
      "epoch 131/1000 , Train Loss : 0.2974763512611389 , Test Loss : 0.30279427766799927\n",
      "epoch 132/1000 , Train Loss : 0.29625204205513 , Test Loss : 0.3015710413455963\n",
      "epoch 133/1000 , Train Loss : 0.29503992199897766 , Test Loss : 0.30035945773124695\n",
      "epoch 134/1000 , Train Loss : 0.29383978247642517 , Test Loss : 0.29915931820869446\n",
      "epoch 135/1000 , Train Loss : 0.2926514446735382 , Test Loss : 0.29797059297561646\n",
      "epoch 136/1000 , Train Loss : 0.2914747893810272 , Test Loss : 0.29679304361343384\n",
      "epoch 137/1000 , Train Loss : 0.2903096079826355 , Test Loss : 0.2956264019012451\n",
      "epoch 138/1000 , Train Loss : 0.28915566205978394 , Test Loss : 0.29447072744369507\n",
      "epoch 139/1000 , Train Loss : 0.28801292181015015 , Test Loss : 0.2933257520198822\n",
      "epoch 140/1000 , Train Loss : 0.28688108921051025 , Test Loss : 0.2921913266181946\n",
      "epoch 141/1000 , Train Loss : 0.28576016426086426 , Test Loss : 0.29106730222702026\n",
      "epoch 142/1000 , Train Loss : 0.28464987874031067 , Test Loss : 0.2899536192417145\n",
      "epoch 143/1000 , Train Loss : 0.28355005383491516 , Test Loss : 0.2888500392436981\n",
      "epoch 144/1000 , Train Loss : 0.28246065974235535 , Test Loss : 0.28775644302368164\n",
      "epoch 145/1000 , Train Loss : 0.2813814580440521 , Test Loss : 0.28667277097702026\n",
      "epoch 146/1000 , Train Loss : 0.28031232953071594 , Test Loss : 0.28559887409210205\n",
      "epoch 147/1000 , Train Loss : 0.27925315499305725 , Test Loss : 0.2845345735549927\n",
      "epoch 148/1000 , Train Loss : 0.2782037556171417 , Test Loss : 0.283479779958725\n",
      "epoch 149/1000 , Train Loss : 0.2771640121936798 , Test Loss : 0.2824343144893646\n",
      "epoch 150/1000 , Train Loss : 0.27613380551338196 , Test Loss : 0.28139811754226685\n",
      "epoch 151/1000 , Train Loss : 0.27511295676231384 , Test Loss : 0.2803710997104645\n",
      "epoch 152/1000 , Train Loss : 0.2741013765335083 , Test Loss : 0.2793530821800232\n",
      "epoch 153/1000 , Train Loss : 0.27309897541999817 , Test Loss : 0.27834397554397583\n",
      "epoch 154/1000 , Train Loss : 0.27210554480552673 , Test Loss : 0.27734360098838806\n",
      "epoch 155/1000 , Train Loss : 0.27112099528312683 , Test Loss : 0.2763519287109375\n",
      "epoch 156/1000 , Train Loss : 0.2701452374458313 , Test Loss : 0.2753688395023346\n",
      "epoch 157/1000 , Train Loss : 0.2691781222820282 , Test Loss : 0.2743941843509674\n",
      "epoch 158/1000 , Train Loss : 0.26821956038475037 , Test Loss : 0.2734278440475464\n",
      "epoch 159/1000 , Train Loss : 0.2672693729400635 , Test Loss : 0.27246978878974915\n",
      "epoch 160/1000 , Train Loss : 0.26632755994796753 , Test Loss : 0.27151986956596375\n",
      "epoch 161/1000 , Train Loss : 0.2653938829898834 , Test Loss : 0.27057796716690063\n",
      "epoch 162/1000 , Train Loss : 0.26446834206581116 , Test Loss : 0.26964399218559265\n",
      "epoch 163/1000 , Train Loss : 0.2635507583618164 , Test Loss : 0.26871779561042786\n",
      "epoch 164/1000 , Train Loss : 0.262641042470932 , Test Loss : 0.2677994668483734\n",
      "epoch 165/1000 , Train Loss : 0.2617391049861908 , Test Loss : 0.26688870787620544\n",
      "epoch 166/1000 , Train Loss : 0.260844886302948 , Test Loss : 0.26598551869392395\n",
      "epoch 167/1000 , Train Loss : 0.2599582076072693 , Test Loss : 0.2650897800922394\n",
      "epoch 168/1000 , Train Loss : 0.2590790390968323 , Test Loss : 0.26420140266418457\n",
      "epoch 169/1000 , Train Loss : 0.258207231760025 , Test Loss : 0.26332032680511475\n",
      "epoch 170/1000 , Train Loss : 0.25734272599220276 , Test Loss : 0.26244640350341797\n",
      "epoch 171/1000 , Train Loss : 0.2564854025840759 , Test Loss : 0.26157963275909424\n",
      "epoch 172/1000 , Train Loss : 0.25563520193099976 , Test Loss : 0.2607198655605316\n",
      "epoch 173/1000 , Train Loss : 0.2547920048236847 , Test Loss : 0.2598671019077301\n",
      "epoch 174/1000 , Train Loss : 0.2539557218551636 , Test Loss : 0.2590211033821106\n",
      "epoch 175/1000 , Train Loss : 0.2531262934207916 , Test Loss : 0.2581818997859955\n",
      "epoch 176/1000 , Train Loss : 0.2523036003112793 , Test Loss : 0.25734943151474\n",
      "epoch 177/1000 , Train Loss : 0.251487672328949 , Test Loss : 0.2565235495567322\n",
      "epoch 178/1000 , Train Loss : 0.25067824125289917 , Test Loss : 0.25570422410964966\n",
      "epoch 179/1000 , Train Loss : 0.24987535178661346 , Test Loss : 0.25489139556884766\n",
      "epoch 180/1000 , Train Loss : 0.24907885491847992 , Test Loss : 0.2540849447250366\n",
      "epoch 181/1000 , Train Loss : 0.24828876554965973 , Test Loss : 0.25328487157821655\n",
      "epoch 182/1000 , Train Loss : 0.24750491976737976 , Test Loss : 0.25249093770980835\n",
      "epoch 183/1000 , Train Loss : 0.24672728776931763 , Test Loss : 0.2517032027244568\n",
      "epoch 184/1000 , Train Loss : 0.24595576524734497 , Test Loss : 0.2509216368198395\n",
      "epoch 185/1000 , Train Loss : 0.2451903074979782 , Test Loss : 0.2501460611820221\n",
      "epoch 186/1000 , Train Loss : 0.2444307953119278 , Test Loss : 0.24937649071216583\n",
      "epoch 187/1000 , Train Loss : 0.24367719888687134 , Test Loss : 0.24861276149749756\n",
      "epoch 188/1000 , Train Loss : 0.24292947351932526 , Test Loss : 0.24785493314266205\n",
      "epoch 189/1000 , Train Loss : 0.2421874850988388 , Test Loss : 0.24710287153720856\n",
      "epoch 190/1000 , Train Loss : 0.2414512038230896 , Test Loss : 0.24635648727416992\n",
      "epoch 191/1000 , Train Loss : 0.24072058498859406 , Test Loss : 0.24561575055122375\n",
      "epoch 192/1000 , Train Loss : 0.23999546468257904 , Test Loss : 0.24488061666488647\n",
      "epoch 193/1000 , Train Loss : 0.23927588760852814 , Test Loss : 0.24415099620819092\n",
      "epoch 194/1000 , Train Loss : 0.2385617345571518 , Test Loss : 0.2434268444776535\n",
      "epoch 195/1000 , Train Loss : 0.23785300552845 , Test Loss : 0.24270807206630707\n",
      "epoch 196/1000 , Train Loss : 0.23714955151081085 , Test Loss : 0.2419946938753128\n",
      "epoch 197/1000 , Train Loss : 0.23645132780075073 , Test Loss : 0.24128656089305878\n",
      "epoch 198/1000 , Train Loss : 0.23575833439826965 , Test Loss : 0.2405836582183838\n",
      "epoch 199/1000 , Train Loss : 0.23507046699523926 , Test Loss : 0.23988597095012665\n",
      "epoch 200/1000 , Train Loss : 0.23438768088817596 , Test Loss : 0.2391933649778366\n",
      "epoch 201/1000 , Train Loss : 0.2337099313735962 , Test Loss : 0.23850585520267487\n",
      "epoch 202/1000 , Train Loss : 0.2330370992422104 , Test Loss : 0.23782332241535187\n",
      "epoch 203/1000 , Train Loss : 0.23236919939517975 , Test Loss : 0.2371457815170288\n",
      "epoch 204/1000 , Train Loss : 0.2317061424255371 , Test Loss : 0.23647314310073853\n",
      "epoch 205/1000 , Train Loss : 0.23104789853096008 , Test Loss : 0.23580534756183624\n",
      "epoch 206/1000 , Train Loss : 0.2303944081068039 , Test Loss : 0.23514236509799957\n",
      "epoch 207/1000 , Train Loss : 0.22974558174610138 , Test Loss : 0.23448412120342255\n",
      "epoch 208/1000 , Train Loss : 0.22910140454769135 , Test Loss : 0.23383063077926636\n",
      "epoch 209/1000 , Train Loss : 0.2284618318080902 , Test Loss : 0.23318174481391907\n",
      "epoch 210/1000 , Train Loss : 0.2278268039226532 , Test Loss : 0.23253749310970306\n",
      "epoch 211/1000 , Train Loss : 0.22719626128673553 , Test Loss : 0.23189781606197357\n",
      "epoch 212/1000 , Train Loss : 0.22657015919685364 , Test Loss : 0.23126263916492462\n",
      "epoch 213/1000 , Train Loss : 0.22594845294952393 , Test Loss : 0.23063194751739502\n",
      "epoch 214/1000 , Train Loss : 0.22533109784126282 , Test Loss : 0.2300056666135788\n",
      "epoch 215/1000 , Train Loss : 0.22471800446510315 , Test Loss : 0.22938379645347595\n",
      "epoch 216/1000 , Train Loss : 0.2241092175245285 , Test Loss : 0.22876621782779694\n",
      "epoch 217/1000 , Train Loss : 0.22350461781024933 , Test Loss : 0.22815296053886414\n",
      "epoch 218/1000 , Train Loss : 0.22290422022342682 , Test Loss : 0.22754395008087158\n",
      "epoch 219/1000 , Train Loss : 0.22230789065361023 , Test Loss : 0.2269391566514969\n",
      "epoch 220/1000 , Train Loss : 0.22171567380428314 , Test Loss : 0.22633853554725647\n",
      "epoch 221/1000 , Train Loss : 0.22112751007080078 , Test Loss : 0.22574201226234436\n",
      "epoch 222/1000 , Train Loss : 0.22054332494735718 , Test Loss : 0.22514961659908295\n",
      "epoch 223/1000 , Train Loss : 0.21996307373046875 , Test Loss : 0.22456122934818268\n",
      "epoch 224/1000 , Train Loss : 0.2193867266178131 , Test Loss : 0.22397686541080475\n",
      "epoch 225/1000 , Train Loss : 0.21881428360939026 , Test Loss : 0.22339648008346558\n",
      "epoch 226/1000 , Train Loss : 0.21824565529823303 , Test Loss : 0.222819983959198\n",
      "epoch 227/1000 , Train Loss : 0.21768082678318024 , Test Loss : 0.2222474217414856\n",
      "epoch 228/1000 , Train Loss : 0.21711976826190948 , Test Loss : 0.2216786891222\n",
      "epoch 229/1000 , Train Loss : 0.2165624350309372 , Test Loss : 0.22111380100250244\n",
      "epoch 230/1000 , Train Loss : 0.2160087376832962 , Test Loss : 0.22055263817310333\n",
      "epoch 231/1000 , Train Loss : 0.2154587060213089 , Test Loss : 0.21999526023864746\n",
      "epoch 232/1000 , Train Loss : 0.21491225063800812 , Test Loss : 0.21944157779216766\n",
      "epoch 233/1000 , Train Loss : 0.21436938643455505 , Test Loss : 0.21889157593250275\n",
      "epoch 234/1000 , Train Loss : 0.21383008360862732 , Test Loss : 0.21834518015384674\n",
      "epoch 235/1000 , Train Loss : 0.21329423785209656 , Test Loss : 0.21780243515968323\n",
      "epoch 236/1000 , Train Loss : 0.21276187896728516 , Test Loss : 0.21726325154304504\n",
      "epoch 237/1000 , Train Loss : 0.21223288774490356 , Test Loss : 0.2167275846004486\n",
      "epoch 238/1000 , Train Loss : 0.21170736849308014 , Test Loss : 0.21619541943073273\n",
      "epoch 239/1000 , Train Loss : 0.21118520200252533 , Test Loss : 0.2156667560338974\n",
      "epoch 240/1000 , Train Loss : 0.21066634356975555 , Test Loss : 0.21514150500297546\n",
      "epoch 241/1000 , Train Loss : 0.21015076339244843 , Test Loss : 0.21461966633796692\n",
      "epoch 242/1000 , Train Loss : 0.20963849127292633 , Test Loss : 0.21410119533538818\n",
      "epoch 243/1000 , Train Loss : 0.2091294229030609 , Test Loss : 0.21358607709407806\n",
      "epoch 244/1000 , Train Loss : 0.20862354338169098 , Test Loss : 0.21307426691055298\n",
      "epoch 245/1000 , Train Loss : 0.20812083780765533 , Test Loss : 0.21256576478481293\n",
      "epoch 246/1000 , Train Loss : 0.20762132108211517 , Test Loss : 0.21206048130989075\n",
      "epoch 247/1000 , Train Loss : 0.20712485909461975 , Test Loss : 0.21155844628810883\n",
      "epoch 248/1000 , Train Loss : 0.20663152635097504 , Test Loss : 0.2110596001148224\n",
      "epoch 249/1000 , Train Loss : 0.20614120364189148 , Test Loss : 0.21056389808654785\n",
      "epoch 250/1000 , Train Loss : 0.20565393567085266 , Test Loss : 0.21007134020328522\n",
      "epoch 251/1000 , Train Loss : 0.2051696479320526 , Test Loss : 0.2095819115638733\n",
      "epoch 252/1000 , Train Loss : 0.20468831062316895 , Test Loss : 0.2090955525636673\n",
      "epoch 253/1000 , Train Loss : 0.20420995354652405 , Test Loss : 0.20861226320266724\n",
      "epoch 254/1000 , Train Loss : 0.20373445749282837 , Test Loss : 0.20813196897506714\n",
      "epoch 255/1000 , Train Loss : 0.20326188206672668 , Test Loss : 0.2076546996831894\n",
      "epoch 256/1000 , Train Loss : 0.20279212296009064 , Test Loss : 0.20718038082122803\n",
      "epoch 257/1000 , Train Loss : 0.202325239777565 , Test Loss : 0.20670902729034424\n",
      "epoch 258/1000 , Train Loss : 0.20186114311218262 , Test Loss : 0.20624057948589325\n",
      "epoch 259/1000 , Train Loss : 0.20139983296394348 , Test Loss : 0.20577503740787506\n",
      "epoch 260/1000 , Train Loss : 0.200941264629364 , Test Loss : 0.20531237125396729\n",
      "epoch 261/1000 , Train Loss : 0.20048540830612183 , Test Loss : 0.20485253632068634\n",
      "epoch 262/1000 , Train Loss : 0.2000322788953781 , Test Loss : 0.20439551770687103\n",
      "epoch 263/1000 , Train Loss : 0.1995818167924881 , Test Loss : 0.20394130051136017\n",
      "epoch 264/1000 , Train Loss : 0.1991340070962906 , Test Loss : 0.20348986983299255\n",
      "epoch 265/1000 , Train Loss : 0.19868884980678558 , Test Loss : 0.2030411660671234\n",
      "epoch 266/1000 , Train Loss : 0.1982463002204895 , Test Loss : 0.20259517431259155\n",
      "epoch 267/1000 , Train Loss : 0.19780631363391876 , Test Loss : 0.20215190947055817\n",
      "epoch 268/1000 , Train Loss : 0.19736890494823456 , Test Loss : 0.20171131193637848\n",
      "epoch 269/1000 , Train Loss : 0.19693399965763092 , Test Loss : 0.2012733668088913\n",
      "epoch 270/1000 , Train Loss : 0.19650164246559143 , Test Loss : 0.20083804428577423\n",
      "epoch 271/1000 , Train Loss : 0.19607175886631012 , Test Loss : 0.20040534436702728\n",
      "epoch 272/1000 , Train Loss : 0.19564440846443176 , Test Loss : 0.19997522234916687\n",
      "epoch 273/1000 , Train Loss : 0.19521944224834442 , Test Loss : 0.1995476633310318\n",
      "epoch 274/1000 , Train Loss : 0.19479690492153168 , Test Loss : 0.19912265241146088\n",
      "epoch 275/1000 , Train Loss : 0.19437679648399353 , Test Loss : 0.1987001597881317\n",
      "epoch 276/1000 , Train Loss : 0.1939590722322464 , Test Loss : 0.19828015565872192\n",
      "epoch 277/1000 , Train Loss : 0.1935437023639679 , Test Loss : 0.1978626698255539\n",
      "epoch 278/1000 , Train Loss : 0.19313065707683563 , Test Loss : 0.19744758307933807\n",
      "epoch 279/1000 , Train Loss : 0.19271999597549438 , Test Loss : 0.19703498482704163\n",
      "epoch 280/1000 , Train Loss : 0.19231157004833221 , Test Loss : 0.1966247856616974\n",
      "epoch 281/1000 , Train Loss : 0.19190548360347748 , Test Loss : 0.19621698558330536\n",
      "epoch 282/1000 , Train Loss : 0.19150160253047943 , Test Loss : 0.19581155478954315\n",
      "epoch 283/1000 , Train Loss : 0.19110001623630524 , Test Loss : 0.19540849328041077\n",
      "epoch 284/1000 , Train Loss : 0.19070066511631012 , Test Loss : 0.19500775635242462\n",
      "epoch 285/1000 , Train Loss : 0.1903034895658493 , Test Loss : 0.1946093589067459\n",
      "epoch 286/1000 , Train Loss : 0.18990851938724518 , Test Loss : 0.19421325623989105\n",
      "epoch 287/1000 , Train Loss : 0.18951570987701416 , Test Loss : 0.19381941854953766\n",
      "epoch 288/1000 , Train Loss : 0.18912506103515625 , Test Loss : 0.19342784583568573\n",
      "epoch 289/1000 , Train Loss : 0.18873655796051025 , Test Loss : 0.19303853809833527\n",
      "epoch 290/1000 , Train Loss : 0.1883501261472702 , Test Loss : 0.19265145063400269\n",
      "epoch 291/1000 , Train Loss : 0.18796582520008087 , Test Loss : 0.1922665536403656\n",
      "epoch 292/1000 , Train Loss : 0.18758362531661987 , Test Loss : 0.19188383221626282\n",
      "epoch 293/1000 , Train Loss : 0.18720345199108124 , Test Loss : 0.19150333106517792\n",
      "epoch 294/1000 , Train Loss : 0.18682536482810974 , Test Loss : 0.19112497568130493\n",
      "epoch 295/1000 , Train Loss : 0.18644924461841583 , Test Loss : 0.19074873626232147\n",
      "epoch 296/1000 , Train Loss : 0.18607519567012787 , Test Loss : 0.19037464261054993\n",
      "epoch 297/1000 , Train Loss : 0.1857031285762787 , Test Loss : 0.19000260531902313\n",
      "epoch 298/1000 , Train Loss : 0.1853330433368683 , Test Loss : 0.18963268399238586\n",
      "epoch 299/1000 , Train Loss : 0.18496491014957428 , Test Loss : 0.18926483392715454\n",
      "epoch 300/1000 , Train Loss : 0.18459874391555786 , Test Loss : 0.18889904022216797\n",
      "epoch 301/1000 , Train Loss : 0.18423449993133545 , Test Loss : 0.18853528797626495\n",
      "epoch 302/1000 , Train Loss : 0.18387216329574585 , Test Loss : 0.18817353248596191\n",
      "epoch 303/1000 , Train Loss : 0.18351173400878906 , Test Loss : 0.18781380355358124\n",
      "epoch 304/1000 , Train Loss : 0.1831531971693039 , Test Loss : 0.18745605647563934\n",
      "epoch 305/1000 , Train Loss : 0.18279653787612915 , Test Loss : 0.18710027635097504\n",
      "epoch 306/1000 , Train Loss : 0.18244172632694244 , Test Loss : 0.18674646317958832\n",
      "epoch 307/1000 , Train Loss : 0.18208874762058258 , Test Loss : 0.186394602060318\n",
      "epoch 308/1000 , Train Loss : 0.18173761665821075 , Test Loss : 0.18604466319084167\n",
      "epoch 309/1000 , Train Loss : 0.18138828873634338 , Test Loss : 0.18569663166999817\n",
      "epoch 310/1000 , Train Loss : 0.1810407191514969 , Test Loss : 0.18535049259662628\n",
      "epoch 311/1000 , Train Loss : 0.18069496750831604 , Test Loss : 0.1850062608718872\n",
      "epoch 312/1000 , Train Loss : 0.18035098910331726 , Test Loss : 0.18466384708881378\n",
      "epoch 313/1000 , Train Loss : 0.18000879883766174 , Test Loss : 0.18432334065437317\n",
      "epoch 314/1000 , Train Loss : 0.17966827750205994 , Test Loss : 0.1839846521615982\n",
      "epoch 315/1000 , Train Loss : 0.1793295443058014 , Test Loss : 0.1836477816104889\n",
      "epoch 316/1000 , Train Loss : 0.17899249494075775 , Test Loss : 0.18331275880336761\n",
      "epoch 317/1000 , Train Loss : 0.1786571741104126 , Test Loss : 0.1829795092344284\n",
      "epoch 318/1000 , Train Loss : 0.17832353711128235 , Test Loss : 0.18264803290367126\n",
      "epoch 319/1000 , Train Loss : 0.17799152433872223 , Test Loss : 0.1823183298110962\n",
      "epoch 320/1000 , Train Loss : 0.1776612401008606 , Test Loss : 0.181990385055542\n",
      "epoch 321/1000 , Train Loss : 0.17733259499073029 , Test Loss : 0.18166416883468628\n",
      "epoch 322/1000 , Train Loss : 0.1770055592060089 , Test Loss : 0.18133972585201263\n",
      "epoch 323/1000 , Train Loss : 0.17668014764785767 , Test Loss : 0.1810169667005539\n",
      "epoch 324/1000 , Train Loss : 0.17635636031627655 , Test Loss : 0.18069593608379364\n",
      "epoch 325/1000 , Train Loss : 0.17603418231010437 , Test Loss : 0.1803765743970871\n",
      "epoch 326/1000 , Train Loss : 0.17571359872817993 , Test Loss : 0.18005889654159546\n",
      "epoch 327/1000 , Train Loss : 0.17539457976818085 , Test Loss : 0.17974288761615753\n",
      "epoch 328/1000 , Train Loss : 0.17507711052894592 , Test Loss : 0.17942851781845093\n",
      "epoch 329/1000 , Train Loss : 0.17476119101047516 , Test Loss : 0.17911580204963684\n",
      "epoch 330/1000 , Train Loss : 0.17444683611392975 , Test Loss : 0.17880471050739288\n",
      "epoch 331/1000 , Train Loss : 0.1741340160369873 , Test Loss : 0.17849527299404144\n",
      "epoch 332/1000 , Train Loss : 0.17382268607616425 , Test Loss : 0.17818738520145416\n",
      "epoch 333/1000 , Train Loss : 0.17351289093494415 , Test Loss : 0.177881121635437\n",
      "epoch 334/1000 , Train Loss : 0.17320457100868225 , Test Loss : 0.1775764375925064\n",
      "epoch 335/1000 , Train Loss : 0.17289772629737854 , Test Loss : 0.17727331817150116\n",
      "epoch 336/1000 , Train Loss : 0.1725923717021942 , Test Loss : 0.17697176337242126\n",
      "epoch 337/1000 , Train Loss : 0.17228849232196808 , Test Loss : 0.17667172849178314\n",
      "epoch 338/1000 , Train Loss : 0.17198604345321655 , Test Loss : 0.17637327313423157\n",
      "epoch 339/1000 , Train Loss : 0.17168503999710083 , Test Loss : 0.17607629299163818\n",
      "epoch 340/1000 , Train Loss : 0.17138546705245972 , Test Loss : 0.17578087747097015\n",
      "epoch 341/1000 , Train Loss : 0.1710873246192932 , Test Loss : 0.17548690736293793\n",
      "epoch 342/1000 , Train Loss : 0.17079058289527893 , Test Loss : 0.17519450187683105\n",
      "epoch 343/1000 , Train Loss : 0.17049525678157806 , Test Loss : 0.1749035120010376\n",
      "epoch 344/1000 , Train Loss : 0.17020130157470703 , Test Loss : 0.17461399734020233\n",
      "epoch 345/1000 , Train Loss : 0.16990870237350464 , Test Loss : 0.17432597279548645\n",
      "epoch 346/1000 , Train Loss : 0.16961750388145447 , Test Loss : 0.17403937876224518\n",
      "epoch 347/1000 , Train Loss : 0.16932767629623413 , Test Loss : 0.1737542301416397\n",
      "epoch 348/1000 , Train Loss : 0.16903918981552124 , Test Loss : 0.17347049713134766\n",
      "epoch 349/1000 , Train Loss : 0.1687520444393158 , Test Loss : 0.1731882095336914\n",
      "epoch 350/1000 , Train Loss : 0.1684662103652954 , Test Loss : 0.172907292842865\n",
      "epoch 351/1000 , Train Loss : 0.16818171739578247 , Test Loss : 0.172627791762352\n",
      "epoch 352/1000 , Train Loss : 0.1678985059261322 , Test Loss : 0.17234966158866882\n",
      "epoch 353/1000 , Train Loss : 0.16761663556098938 , Test Loss : 0.17207291722297668\n",
      "epoch 354/1000 , Train Loss : 0.16733606159687042 , Test Loss : 0.17179754376411438\n",
      "epoch 355/1000 , Train Loss : 0.16705673933029175 , Test Loss : 0.1715235412120819\n",
      "epoch 356/1000 , Train Loss : 0.16677868366241455 , Test Loss : 0.17125087976455688\n",
      "epoch 357/1000 , Train Loss : 0.1665019392967224 , Test Loss : 0.17097952961921692\n",
      "epoch 358/1000 , Train Loss : 0.16622643172740936 , Test Loss : 0.1707095205783844\n",
      "epoch 359/1000 , Train Loss : 0.1659521609544754 , Test Loss : 0.17044086754322052\n",
      "epoch 360/1000 , Train Loss : 0.16567915678024292 , Test Loss : 0.1701734960079193\n",
      "epoch 361/1000 , Train Loss : 0.16540735960006714 , Test Loss : 0.16990742087364197\n",
      "epoch 362/1000 , Train Loss : 0.16513679921627045 , Test Loss : 0.16964265704154968\n",
      "epoch 363/1000 , Train Loss : 0.16486744582653046 , Test Loss : 0.16937915980815887\n",
      "epoch 364/1000 , Train Loss : 0.16459926962852478 , Test Loss : 0.16911694407463074\n",
      "epoch 365/1000 , Train Loss : 0.1643323302268982 , Test Loss : 0.16885599493980408\n",
      "epoch 366/1000 , Train Loss : 0.1640665978193283 , Test Loss : 0.1685962826013565\n",
      "epoch 367/1000 , Train Loss : 0.16380202770233154 , Test Loss : 0.1683378368616104\n",
      "epoch 368/1000 , Train Loss : 0.1635386049747467 , Test Loss : 0.1680806428194046\n",
      "epoch 369/1000 , Train Loss : 0.16327637434005737 , Test Loss : 0.1678246706724167\n",
      "epoch 370/1000 , Train Loss : 0.16301529109477997 , Test Loss : 0.16756990551948547\n",
      "epoch 371/1000 , Train Loss : 0.1627553552389145 , Test Loss : 0.16731634736061096\n",
      "epoch 372/1000 , Train Loss : 0.16249658167362213 , Test Loss : 0.16706402599811554\n",
      "epoch 373/1000 , Train Loss : 0.16223891079425812 , Test Loss : 0.16681286692619324\n",
      "epoch 374/1000 , Train Loss : 0.16198238730430603 , Test Loss : 0.16656292974948883\n",
      "epoch 375/1000 , Train Loss : 0.16172698140144348 , Test Loss : 0.16631418466567993\n",
      "epoch 376/1000 , Train Loss : 0.16147269308567047 , Test Loss : 0.16606655716896057\n",
      "epoch 377/1000 , Train Loss : 0.1612194925546646 , Test Loss : 0.1658201366662979\n",
      "epoch 378/1000 , Train Loss : 0.1609673947095871 , Test Loss : 0.16557486355304718\n",
      "epoch 379/1000 , Train Loss : 0.16071636974811554 , Test Loss : 0.16533072292804718\n",
      "epoch 380/1000 , Train Loss : 0.16046644747257233 , Test Loss : 0.1650877743959427\n",
      "epoch 381/1000 , Train Loss : 0.16021761298179626 , Test Loss : 0.16484592854976654\n",
      "epoch 382/1000 , Train Loss : 0.15996980667114258 , Test Loss : 0.16460521519184113\n",
      "epoch 383/1000 , Train Loss : 0.15972310304641724 , Test Loss : 0.16436557471752167\n",
      "epoch 384/1000 , Train Loss : 0.15947742760181427 , Test Loss : 0.16412711143493652\n",
      "epoch 385/1000 , Train Loss : 0.15923281013965607 , Test Loss : 0.16388970613479614\n",
      "epoch 386/1000 , Train Loss : 0.15898922085762024 , Test Loss : 0.1636534184217453\n",
      "epoch 387/1000 , Train Loss : 0.1587466597557068 , Test Loss : 0.16341820359230042\n",
      "epoch 388/1000 , Train Loss : 0.1585051417350769 , Test Loss : 0.16318407654762268\n",
      "epoch 389/1000 , Train Loss : 0.158264622092247 , Test Loss : 0.1629510521888733\n",
      "epoch 390/1000 , Train Loss : 0.15802514553070068 , Test Loss : 0.16271907091140747\n",
      "epoch 391/1000 , Train Loss : 0.15778666734695435 , Test Loss : 0.16248813271522522\n",
      "epoch 392/1000 , Train Loss : 0.1575491726398468 , Test Loss : 0.16225826740264893\n",
      "epoch 393/1000 , Train Loss : 0.15731267631053925 , Test Loss : 0.1620294600725174\n",
      "epoch 394/1000 , Train Loss : 0.15707719326019287 , Test Loss : 0.16180168092250824\n",
      "epoch 395/1000 , Train Loss : 0.1568426638841629 , Test Loss : 0.16157492995262146\n",
      "epoch 396/1000 , Train Loss : 0.15660911798477173 , Test Loss : 0.16134920716285706\n",
      "epoch 397/1000 , Train Loss : 0.15637654066085815 , Test Loss : 0.16112449765205383\n",
      "epoch 398/1000 , Train Loss : 0.156144917011261 , Test Loss : 0.16090081632137299\n",
      "epoch 399/1000 , Train Loss : 0.1559142768383026 , Test Loss : 0.16067813336849213\n",
      "epoch 400/1000 , Train Loss : 0.15568454563617706 , Test Loss : 0.16045644879341125\n",
      "epoch 401/1000 , Train Loss : 0.15545576810836792 , Test Loss : 0.16023574769496918\n",
      "epoch 402/1000 , Train Loss : 0.15522794425487518 , Test Loss : 0.1600160449743271\n",
      "epoch 403/1000 , Train Loss : 0.15500104427337646 , Test Loss : 0.1597973257303238\n",
      "epoch 404/1000 , Train Loss : 0.15477508306503296 , Test Loss : 0.1595795601606369\n",
      "epoch 405/1000 , Train Loss : 0.15455001592636108 , Test Loss : 0.1593627780675888\n",
      "epoch 406/1000 , Train Loss : 0.15432588756084442 , Test Loss : 0.1591469645500183\n",
      "epoch 407/1000 , Train Loss : 0.15410266816616058 , Test Loss : 0.15893208980560303\n",
      "epoch 408/1000 , Train Loss : 0.15388034284114838 , Test Loss : 0.15871818363666534\n",
      "epoch 409/1000 , Train Loss : 0.1536588966846466 , Test Loss : 0.15850521624088287\n",
      "epoch 410/1000 , Train Loss : 0.15343837440013885 , Test Loss : 0.15829315781593323\n",
      "epoch 411/1000 , Train Loss : 0.15321871638298035 , Test Loss : 0.1580820381641388\n",
      "epoch 412/1000 , Train Loss : 0.15299995243549347 , Test Loss : 0.15787188708782196\n",
      "epoch 413/1000 , Train Loss : 0.15278206765651703 , Test Loss : 0.15766260027885437\n",
      "epoch 414/1000 , Train Loss : 0.15256506204605103 , Test Loss : 0.15745426714420319\n",
      "epoch 415/1000 , Train Loss : 0.15234887599945068 , Test Loss : 0.15724679827690125\n",
      "epoch 416/1000 , Train Loss : 0.15213359892368317 , Test Loss : 0.15704025328159332\n",
      "epoch 417/1000 , Train Loss : 0.1519191414117813 , Test Loss : 0.15683461725711823\n",
      "epoch 418/1000 , Train Loss : 0.1517055481672287 , Test Loss : 0.15662987530231476\n",
      "epoch 419/1000 , Train Loss : 0.15149278938770294 , Test Loss : 0.15642598271369934\n",
      "epoch 420/1000 , Train Loss : 0.15128089487552643 , Test Loss : 0.15622298419475555\n",
      "epoch 421/1000 , Train Loss : 0.15106980502605438 , Test Loss : 0.1560208946466446\n",
      "epoch 422/1000 , Train Loss : 0.1508595496416092 , Test Loss : 0.15581963956356049\n",
      "epoch 423/1000 , Train Loss : 0.15065012872219086 , Test Loss : 0.15561924874782562\n",
      "epoch 424/1000 , Train Loss : 0.150441512465477 , Test Loss : 0.1554197072982788\n",
      "epoch 425/1000 , Train Loss : 0.15023371577262878 , Test Loss : 0.15522103011608124\n",
      "epoch 426/1000 , Train Loss : 0.15002672374248505 , Test Loss : 0.15502318739891052\n",
      "epoch 427/1000 , Train Loss : 0.14982053637504578 , Test Loss : 0.15482622385025024\n",
      "epoch 428/1000 , Train Loss : 0.14961515367031097 , Test Loss : 0.15463005006313324\n",
      "epoch 429/1000 , Train Loss : 0.14941057562828064 , Test Loss : 0.15443472564220428\n",
      "epoch 430/1000 , Train Loss : 0.14920678734779358 , Test Loss : 0.154240220785141\n",
      "epoch 431/1000 , Train Loss : 0.1490037441253662 , Test Loss : 0.15404655039310455\n",
      "epoch 432/1000 , Train Loss : 0.1488015204668045 , Test Loss : 0.1538536697626114\n",
      "epoch 433/1000 , Train Loss : 0.1486000418663025 , Test Loss : 0.15366163849830627\n",
      "epoch 434/1000 , Train Loss : 0.14839936792850494 , Test Loss : 0.15347038209438324\n",
      "epoch 435/1000 , Train Loss : 0.1481994390487671 , Test Loss : 0.15327994525432587\n",
      "epoch 436/1000 , Train Loss : 0.14800028502941132 , Test Loss : 0.15309029817581177\n",
      "epoch 437/1000 , Train Loss : 0.14780189096927643 , Test Loss : 0.15290144085884094\n",
      "epoch 438/1000 , Train Loss : 0.14760421216487885 , Test Loss : 0.1527133733034134\n",
      "epoch 439/1000 , Train Loss : 0.14740732312202454 , Test Loss : 0.1525260955095291\n",
      "epoch 440/1000 , Train Loss : 0.14721116423606873 , Test Loss : 0.15233959257602692\n",
      "epoch 441/1000 , Train Loss : 0.1470157504081726 , Test Loss : 0.1521538347005844\n",
      "epoch 442/1000 , Train Loss : 0.1468210518360138 , Test Loss : 0.15196888148784637\n",
      "epoch 443/1000 , Train Loss : 0.14662711322307587 , Test Loss : 0.15178465843200684\n",
      "epoch 444/1000 , Train Loss : 0.14643386006355286 , Test Loss : 0.15160121023654938\n",
      "epoch 445/1000 , Train Loss : 0.14624138176441193 , Test Loss : 0.1514185070991516\n",
      "epoch 446/1000 , Train Loss : 0.14604957401752472 , Test Loss : 0.15123657882213593\n",
      "epoch 447/1000 , Train Loss : 0.1458585262298584 , Test Loss : 0.15105538070201874\n",
      "epoch 448/1000 , Train Loss : 0.145668163895607 , Test Loss : 0.15087491273880005\n",
      "epoch 449/1000 , Train Loss : 0.1454785019159317 , Test Loss : 0.15069518983364105\n",
      "epoch 450/1000 , Train Loss : 0.1452895551919937 , Test Loss : 0.15051616728305817\n",
      "epoch 451/1000 , Train Loss : 0.14510130882263184 , Test Loss : 0.15033790469169617\n",
      "epoch 452/1000 , Train Loss : 0.14491373300552368 , Test Loss : 0.15016037225723267\n",
      "epoch 453/1000 , Train Loss : 0.14472688734531403 , Test Loss : 0.14998352527618408\n",
      "epoch 454/1000 , Train Loss : 0.1445406973361969 , Test Loss : 0.14980743825435638\n",
      "epoch 455/1000 , Train Loss : 0.1443551927804947 , Test Loss : 0.1496320366859436\n",
      "epoch 456/1000 , Train Loss : 0.1441703885793686 , Test Loss : 0.14945732057094574\n",
      "epoch 457/1000 , Train Loss : 0.14398622512817383 , Test Loss : 0.14928333461284637\n",
      "epoch 458/1000 , Train Loss : 0.14380276203155518 , Test Loss : 0.14911004900932312\n",
      "epoch 459/1000 , Train Loss : 0.14361995458602905 , Test Loss : 0.1489374339580536\n",
      "epoch 460/1000 , Train Loss : 0.14343780279159546 , Test Loss : 0.14876551926136017\n",
      "epoch 461/1000 , Train Loss : 0.1432563066482544 , Test Loss : 0.14859427511692047\n",
      "epoch 462/1000 , Train Loss : 0.14307548105716705 , Test Loss : 0.14842373132705688\n",
      "epoch 463/1000 , Train Loss : 0.14289529621601105 , Test Loss : 0.1482538878917694\n",
      "epoch 464/1000 , Train Loss : 0.14271578192710876 , Test Loss : 0.14808467030525208\n",
      "epoch 465/1000 , Train Loss : 0.14253689348697662 , Test Loss : 0.14791613817214966\n",
      "epoch 466/1000 , Train Loss : 0.14235864579677582 , Test Loss : 0.14774829149246216\n",
      "epoch 467/1000 , Train Loss : 0.14218105375766754 , Test Loss : 0.147581085562706\n",
      "epoch 468/1000 , Train Loss : 0.14200405776500702 , Test Loss : 0.14741455018520355\n",
      "epoch 469/1000 , Train Loss : 0.1418277472257614 , Test Loss : 0.14724865555763245\n",
      "epoch 470/1000 , Train Loss : 0.14165201783180237 , Test Loss : 0.14708341658115387\n",
      "epoch 471/1000 , Train Loss : 0.14147691428661346 , Test Loss : 0.14691881835460663\n",
      "epoch 472/1000 , Train Loss : 0.1413024663925171 , Test Loss : 0.14675487577915192\n",
      "epoch 473/1000 , Train Loss : 0.14112861454486847 , Test Loss : 0.14659155905246735\n",
      "epoch 474/1000 , Train Loss : 0.1409553736448288 , Test Loss : 0.1464288979768753\n",
      "epoch 475/1000 , Train Loss : 0.14078275859355927 , Test Loss : 0.1462668627500534\n",
      "epoch 476/1000 , Train Loss : 0.1406107395887375 , Test Loss : 0.14610545337200165\n",
      "epoch 477/1000 , Train Loss : 0.14043931663036346 , Test Loss : 0.14594466984272003\n",
      "epoch 478/1000 , Train Loss : 0.1402684897184372 , Test Loss : 0.14578449726104736\n",
      "epoch 479/1000 , Train Loss : 0.14009828865528107 , Test Loss : 0.14562492072582245\n",
      "epoch 480/1000 , Train Loss : 0.1399286687374115 , Test Loss : 0.14546598494052887\n",
      "epoch 481/1000 , Train Loss : 0.1397596299648285 , Test Loss : 0.14530767500400543\n",
      "epoch 482/1000 , Train Loss : 0.13959118723869324 , Test Loss : 0.14514997601509094\n",
      "epoch 483/1000 , Train Loss : 0.13942332565784454 , Test Loss : 0.144992858171463\n",
      "epoch 484/1000 , Train Loss : 0.1392560452222824 , Test Loss : 0.14483635127544403\n",
      "epoch 485/1000 , Train Loss : 0.13908933103084564 , Test Loss : 0.1446804255247116\n",
      "epoch 486/1000 , Train Loss : 0.13892322778701782 , Test Loss : 0.14452511072158813\n",
      "epoch 487/1000 , Train Loss : 0.13875766098499298 , Test Loss : 0.14437036216259003\n",
      "epoch 488/1000 , Train Loss : 0.1385926753282547 , Test Loss : 0.14421622455120087\n",
      "epoch 489/1000 , Train Loss : 0.13842825591564178 , Test Loss : 0.14406268298625946\n",
      "epoch 490/1000 , Train Loss : 0.13826440274715424 , Test Loss : 0.14390969276428223\n",
      "epoch 491/1000 , Train Loss : 0.13810111582279205 , Test Loss : 0.14375729858875275\n",
      "epoch 492/1000 , Train Loss : 0.13793835043907166 , Test Loss : 0.14360545575618744\n",
      "epoch 493/1000 , Train Loss : 0.13777616620063782 , Test Loss : 0.14345420897006989\n",
      "epoch 494/1000 , Train Loss : 0.13761454820632935 , Test Loss : 0.1433035284280777\n",
      "epoch 495/1000 , Train Loss : 0.13745345175266266 , Test Loss : 0.14315339922904968\n",
      "epoch 496/1000 , Train Loss : 0.13729290664196014 , Test Loss : 0.14300383627414703\n",
      "epoch 497/1000 , Train Loss : 0.137132927775383 , Test Loss : 0.14285483956336975\n",
      "epoch 498/1000 , Train Loss : 0.13697345554828644 , Test Loss : 0.14270639419555664\n",
      "epoch 499/1000 , Train Loss : 0.13681451976299286 , Test Loss : 0.1425584852695465\n",
      "epoch 500/1000 , Train Loss : 0.13665613532066345 , Test Loss : 0.14241115748882294\n",
      "epoch 501/1000 , Train Loss : 0.13649828732013702 , Test Loss : 0.14226435124874115\n",
      "epoch 502/1000 , Train Loss : 0.1363409459590912 , Test Loss : 0.14211809635162354\n",
      "epoch 503/1000 , Train Loss : 0.13618415594100952 , Test Loss : 0.1419723778963089\n",
      "epoch 504/1000 , Train Loss : 0.13602787256240845 , Test Loss : 0.14182719588279724\n",
      "epoch 505/1000 , Train Loss : 0.13587209582328796 , Test Loss : 0.14168255031108856\n",
      "epoch 506/1000 , Train Loss : 0.13571688532829285 , Test Loss : 0.14153844118118286\n",
      "epoch 507/1000 , Train Loss : 0.13556215167045593 , Test Loss : 0.14139486849308014\n",
      "epoch 508/1000 , Train Loss : 0.1354079246520996 , Test Loss : 0.141251802444458\n",
      "epoch 509/1000 , Train Loss : 0.13525423407554626 , Test Loss : 0.14110927283763885\n",
      "epoch 510/1000 , Train Loss : 0.1351010501384735 , Test Loss : 0.1409672498703003\n",
      "epoch 511/1000 , Train Loss : 0.13494834303855896 , Test Loss : 0.14082574844360352\n",
      "epoch 512/1000 , Train Loss : 0.1347961276769638 , Test Loss : 0.14068478345870972\n",
      "epoch 513/1000 , Train Loss : 0.13464446365833282 , Test Loss : 0.14054431021213531\n",
      "epoch 514/1000 , Train Loss : 0.13449327647686005 , Test Loss : 0.1404043585062027\n",
      "epoch 515/1000 , Train Loss : 0.13434256613254547 , Test Loss : 0.14026489853858948\n",
      "epoch 516/1000 , Train Loss : 0.1341923326253891 , Test Loss : 0.14012596011161804\n",
      "epoch 517/1000 , Train Loss : 0.1340426355600357 , Test Loss : 0.1399874985218048\n",
      "epoch 518/1000 , Train Loss : 0.13389338552951813 , Test Loss : 0.13984955847263336\n",
      "epoch 519/1000 , Train Loss : 0.13374464213848114 , Test Loss : 0.13971209526062012\n",
      "epoch 520/1000 , Train Loss : 0.13359637558460236 , Test Loss : 0.13957513868808746\n",
      "epoch 521/1000 , Train Loss : 0.13344860076904297 , Test Loss : 0.139438658952713\n",
      "epoch 522/1000 , Train Loss : 0.1333012729883194 , Test Loss : 0.13930268585681915\n",
      "epoch 523/1000 , Train Loss : 0.13315443694591522 , Test Loss : 0.1391671746969223\n",
      "epoch 524/1000 , Train Loss : 0.13300806283950806 , Test Loss : 0.13903217017650604\n",
      "epoch 525/1000 , Train Loss : 0.1328621655702591 , Test Loss : 0.13889764249324799\n",
      "epoch 526/1000 , Train Loss : 0.13271674513816833 , Test Loss : 0.13876356184482574\n",
      "epoch 527/1000 , Train Loss : 0.13257178664207458 , Test Loss : 0.1386300027370453\n",
      "epoch 528/1000 , Train Loss : 0.13242729008197784 , Test Loss : 0.13849689066410065\n",
      "epoch 529/1000 , Train Loss : 0.1322832554578781 , Test Loss : 0.1383642554283142\n",
      "epoch 530/1000 , Train Loss : 0.1321396678686142 , Test Loss : 0.13823209702968597\n",
      "epoch 531/1000 , Train Loss : 0.13199655711650848 , Test Loss : 0.13810038566589355\n",
      "epoch 532/1000 , Train Loss : 0.1318538784980774 , Test Loss : 0.13796912133693695\n",
      "epoch 533/1000 , Train Loss : 0.1317116767168045 , Test Loss : 0.13783834874629974\n",
      "epoch 534/1000 , Train Loss : 0.13156990706920624 , Test Loss : 0.13770805299282074\n",
      "epoch 535/1000 , Train Loss : 0.1314285844564438 , Test Loss : 0.13757817447185516\n",
      "epoch 536/1000 , Train Loss : 0.13128770887851715 , Test Loss : 0.1374487578868866\n",
      "epoch 537/1000 , Train Loss : 0.13114728033542633 , Test Loss : 0.13731980323791504\n",
      "epoch 538/1000 , Train Loss : 0.13100728392601013 , Test Loss : 0.1371912956237793\n",
      "epoch 539/1000 , Train Loss : 0.13086773455142975 , Test Loss : 0.13706323504447937\n",
      "epoch 540/1000 , Train Loss : 0.130728617310524 , Test Loss : 0.13693560659885406\n",
      "epoch 541/1000 , Train Loss : 0.13058990240097046 , Test Loss : 0.13680842518806458\n",
      "epoch 542/1000 , Train Loss : 0.13045166432857513 , Test Loss : 0.1366816759109497\n",
      "epoch 543/1000 , Train Loss : 0.13031384348869324 , Test Loss : 0.13655537366867065\n",
      "epoch 544/1000 , Train Loss : 0.13017643988132477 , Test Loss : 0.13642950356006622\n",
      "epoch 545/1000 , Train Loss : 0.1300394982099533 , Test Loss : 0.1363040804862976\n",
      "epoch 546/1000 , Train Loss : 0.1299029290676117 , Test Loss : 0.13617907464504242\n",
      "epoch 547/1000 , Train Loss : 0.12976683676242828 , Test Loss : 0.13605450093746185\n",
      "epoch 548/1000 , Train Loss : 0.12963111698627472 , Test Loss : 0.13593034446239471\n",
      "epoch 549/1000 , Train Loss : 0.12949584424495697 , Test Loss : 0.1358066201210022\n",
      "epoch 550/1000 , Train Loss : 0.12936097383499146 , Test Loss : 0.1356833130121231\n",
      "epoch 551/1000 , Train Loss : 0.12922652065753937 , Test Loss : 0.13556040823459625\n",
      "epoch 552/1000 , Train Loss : 0.12909246981143951 , Test Loss : 0.1354379504919052\n",
      "epoch 553/1000 , Train Loss : 0.12895885109901428 , Test Loss : 0.1353159099817276\n",
      "epoch 554/1000 , Train Loss : 0.1288256198167801 , Test Loss : 0.13519428670406342\n",
      "epoch 555/1000 , Train Loss : 0.12869279086589813 , Test Loss : 0.13507306575775146\n",
      "epoch 556/1000 , Train Loss : 0.1285603791475296 , Test Loss : 0.13495223224163055\n",
      "epoch 557/1000 , Train Loss : 0.1284283548593521 , Test Loss : 0.13483181595802307\n",
      "epoch 558/1000 , Train Loss : 0.12829676270484924 , Test Loss : 0.1347118318080902\n",
      "epoch 559/1000 , Train Loss : 0.12816554307937622 , Test Loss : 0.1345922350883484\n",
      "epoch 560/1000 , Train Loss : 0.12803472578525543 , Test Loss : 0.1344730406999588\n",
      "epoch 561/1000 , Train Loss : 0.12790429592132568 , Test Loss : 0.13435424864292145\n",
      "epoch 562/1000 , Train Loss : 0.12777428328990936 , Test Loss : 0.13423584401607513\n",
      "epoch 563/1000 , Train Loss : 0.1276446282863617 , Test Loss : 0.13411784172058105\n",
      "epoch 564/1000 , Train Loss : 0.12751537561416626 , Test Loss : 0.13400022685527802\n",
      "epoch 565/1000 , Train Loss : 0.12738651037216187 , Test Loss : 0.13388299942016602\n",
      "epoch 566/1000 , Train Loss : 0.12725801765918732 , Test Loss : 0.13376617431640625\n",
      "epoch 567/1000 , Train Loss : 0.127129927277565 , Test Loss : 0.13364973664283752\n",
      "epoch 568/1000 , Train Loss : 0.12700220942497253 , Test Loss : 0.13353367149829865\n",
      "epoch 569/1000 , Train Loss : 0.1268748790025711 , Test Loss : 0.133418008685112\n",
      "epoch 570/1000 , Train Loss : 0.1267479509115219 , Test Loss : 0.133302703499794\n",
      "epoch 571/1000 , Train Loss : 0.12662136554718018 , Test Loss : 0.13318780064582825\n",
      "epoch 572/1000 , Train Loss : 0.1264951527118683 , Test Loss : 0.13307327032089233\n",
      "epoch 573/1000 , Train Loss : 0.12636932730674744 , Test Loss : 0.13295911252498627\n",
      "epoch 574/1000 , Train Loss : 0.12624385952949524 , Test Loss : 0.13284534215927124\n",
      "epoch 575/1000 , Train Loss : 0.12611879408359528 , Test Loss : 0.13273192942142487\n",
      "epoch 576/1000 , Train Loss : 0.12599404156208038 , Test Loss : 0.13261890411376953\n",
      "epoch 577/1000 , Train Loss : 0.12586969137191772 , Test Loss : 0.13250622153282166\n",
      "epoch 578/1000 , Train Loss : 0.1257457137107849 , Test Loss : 0.13239391148090363\n",
      "epoch 579/1000 , Train Loss : 0.12562207877635956 , Test Loss : 0.13228198885917664\n",
      "epoch 580/1000 , Train Loss : 0.12549881637096405 , Test Loss : 0.1321704089641571\n",
      "epoch 581/1000 , Train Loss : 0.1253759264945984 , Test Loss : 0.13205920159816742\n",
      "epoch 582/1000 , Train Loss : 0.125253364443779 , Test Loss : 0.1319483518600464\n",
      "epoch 583/1000 , Train Loss : 0.12513117492198944 , Test Loss : 0.1318378746509552\n",
      "epoch 584/1000 , Train Loss : 0.12500934302806854 , Test Loss : 0.13172772526741028\n",
      "epoch 585/1000 , Train Loss : 0.1248878613114357 , Test Loss : 0.1316179633140564\n",
      "epoch 586/1000 , Train Loss : 0.12476672232151031 , Test Loss : 0.13150854408740997\n",
      "epoch 587/1000 , Train Loss : 0.12464595586061478 , Test Loss : 0.1313994824886322\n",
      "epoch 588/1000 , Train Loss : 0.12452550232410431 , Test Loss : 0.1312907487154007\n",
      "epoch 589/1000 , Train Loss : 0.12440543621778488 , Test Loss : 0.13118235766887665\n",
      "epoch 590/1000 , Train Loss : 0.12428567558526993 , Test Loss : 0.13107433915138245\n",
      "epoch 591/1000 , Train Loss : 0.12416628003120422 , Test Loss : 0.1309666782617569\n",
      "epoch 592/1000 , Train Loss : 0.12404721975326538 , Test Loss : 0.13085933029651642\n",
      "epoch 593/1000 , Train Loss : 0.1239285096526146 , Test Loss : 0.1307523250579834\n",
      "epoch 594/1000 , Train Loss : 0.12381012737751007 , Test Loss : 0.13064567744731903\n",
      "epoch 595/1000 , Train Loss : 0.123692087829113 , Test Loss : 0.13053937256336212\n",
      "epoch 596/1000 , Train Loss : 0.1235743835568428 , Test Loss : 0.13043338060379028\n",
      "epoch 597/1000 , Train Loss : 0.12345702201128006 , Test Loss : 0.1303277313709259\n",
      "epoch 598/1000 , Train Loss : 0.12333998829126358 , Test Loss : 0.13022242486476898\n",
      "epoch 599/1000 , Train Loss : 0.12322328239679337 , Test Loss : 0.13011744618415833\n",
      "epoch 600/1000 , Train Loss : 0.12310690432786942 , Test Loss : 0.13001281023025513\n",
      "epoch 601/1000 , Train Loss : 0.12299086898565292 , Test Loss : 0.129908487200737\n",
      "epoch 602/1000 , Train Loss : 0.1228751540184021 , Test Loss : 0.12980449199676514\n",
      "epoch 603/1000 , Train Loss : 0.12275976687669754 , Test Loss : 0.12970083951950073\n",
      "epoch 604/1000 , Train Loss : 0.12264470010995865 , Test Loss : 0.1295974850654602\n",
      "epoch 605/1000 , Train Loss : 0.12252994626760483 , Test Loss : 0.12949445843696594\n",
      "epoch 606/1000 , Train Loss : 0.12241552770137787 , Test Loss : 0.12939177453517914\n",
      "epoch 607/1000 , Train Loss : 0.12230142951011658 , Test Loss : 0.1292894184589386\n",
      "epoch 608/1000 , Train Loss : 0.12218765914440155 , Test Loss : 0.12918736040592194\n",
      "epoch 609/1000 , Train Loss : 0.1220741868019104 , Test Loss : 0.12908563017845154\n",
      "epoch 610/1000 , Train Loss : 0.12196102738380432 , Test Loss : 0.12898419797420502\n",
      "epoch 611/1000 , Train Loss : 0.1218481957912445 , Test Loss : 0.12888310849666595\n",
      "epoch 612/1000 , Train Loss : 0.12173566967248917 , Test Loss : 0.12878230214118958\n",
      "epoch 613/1000 , Train Loss : 0.1216234639286995 , Test Loss : 0.12868179380893707\n",
      "epoch 614/1000 , Train Loss : 0.1215115636587143 , Test Loss : 0.12858164310455322\n",
      "epoch 615/1000 , Train Loss : 0.12139996886253357 , Test Loss : 0.12848177552223206\n",
      "epoch 616/1000 , Train Loss : 0.12128868699073792 , Test Loss : 0.12838220596313477\n",
      "epoch 617/1000 , Train Loss : 0.12117771804332733 , Test Loss : 0.12828296422958374\n",
      "epoch 618/1000 , Train Loss : 0.12106705456972122 , Test Loss : 0.1281839907169342\n",
      "epoch 619/1000 , Train Loss : 0.12095668911933899 , Test Loss : 0.12808534502983093\n",
      "epoch 620/1000 , Train Loss : 0.12084661424160004 , Test Loss : 0.12798702716827393\n",
      "epoch 621/1000 , Train Loss : 0.12073684483766556 , Test Loss : 0.1278889924287796\n",
      "epoch 622/1000 , Train Loss : 0.12062740325927734 , Test Loss : 0.12779122591018677\n",
      "epoch 623/1000 , Train Loss : 0.12051822245121002 , Test Loss : 0.1276937872171402\n",
      "epoch 624/1000 , Train Loss : 0.12040936201810837 , Test Loss : 0.1275966465473175\n",
      "epoch 625/1000 , Train Loss : 0.1203007847070694 , Test Loss : 0.1274997889995575\n",
      "epoch 626/1000 , Train Loss : 0.1201925203204155 , Test Loss : 0.12740322947502136\n",
      "epoch 627/1000 , Train Loss : 0.12008452415466309 , Test Loss : 0.12730693817138672\n",
      "epoch 628/1000 , Train Loss : 0.11997684836387634 , Test Loss : 0.12721097469329834\n",
      "epoch 629/1000 , Train Loss : 0.11986944824457169 , Test Loss : 0.12711527943611145\n",
      "epoch 630/1000 , Train Loss : 0.11976233124732971 , Test Loss : 0.12701988220214844\n",
      "epoch 631/1000 , Train Loss : 0.11965551972389221 , Test Loss : 0.1269247829914093\n",
      "epoch 632/1000 , Train Loss : 0.1195489913225174 , Test Loss : 0.12682995200157166\n",
      "epoch 633/1000 , Train Loss : 0.11944274604320526 , Test Loss : 0.1267353892326355\n",
      "epoch 634/1000 , Train Loss : 0.11933679133653641 , Test Loss : 0.12664112448692322\n",
      "epoch 635/1000 , Train Loss : 0.11923109740018845 , Test Loss : 0.12654715776443481\n",
      "epoch 636/1000 , Train Loss : 0.11912570893764496 , Test Loss : 0.1264534592628479\n",
      "epoch 637/1000 , Train Loss : 0.11902060359716415 , Test Loss : 0.12636002898216248\n",
      "epoch 638/1000 , Train Loss : 0.11891576647758484 , Test Loss : 0.12626689672470093\n",
      "epoch 639/1000 , Train Loss : 0.1188112199306488 , Test Loss : 0.12617404758930206\n",
      "epoch 640/1000 , Train Loss : 0.11870692670345306 , Test Loss : 0.1260814517736435\n",
      "epoch 641/1000 , Train Loss : 0.1186029389500618 , Test Loss : 0.1259891539812088\n",
      "epoch 642/1000 , Train Loss : 0.11849921941757202 , Test Loss : 0.1258970946073532\n",
      "epoch 643/1000 , Train Loss : 0.11839577555656433 , Test Loss : 0.1258053332567215\n",
      "epoch 644/1000 , Train Loss : 0.11829259991645813 , Test Loss : 0.12571382522583008\n",
      "epoch 645/1000 , Train Loss : 0.11818969994783401 , Test Loss : 0.12562260031700134\n",
      "epoch 646/1000 , Train Loss : 0.11808706820011139 , Test Loss : 0.1255316585302353\n",
      "epoch 647/1000 , Train Loss : 0.11798469722270966 , Test Loss : 0.12544095516204834\n",
      "epoch 648/1000 , Train Loss : 0.1178826168179512 , Test Loss : 0.12535052001476288\n",
      "epoch 649/1000 , Train Loss : 0.11778078228235245 , Test Loss : 0.1252603679895401\n",
      "epoch 650/1000 , Train Loss : 0.11767922341823578 , Test Loss : 0.12517046928405762\n",
      "epoch 651/1000 , Train Loss : 0.1175779327750206 , Test Loss : 0.12508083879947662\n",
      "epoch 652/1000 , Train Loss : 0.1174769178032875 , Test Loss : 0.12499145418405533\n",
      "epoch 653/1000 , Train Loss : 0.11737614125013351 , Test Loss : 0.12490235269069672\n",
      "epoch 654/1000 , Train Loss : 0.1172756478190422 , Test Loss : 0.12481348216533661\n",
      "epoch 655/1000 , Train Loss : 0.1171754002571106 , Test Loss : 0.12472490221261978\n",
      "epoch 656/1000 , Train Loss : 0.11707542091608047 , Test Loss : 0.12463655322790146\n",
      "epoch 657/1000 , Train Loss : 0.11697570234537125 , Test Loss : 0.12454847246408463\n",
      "epoch 658/1000 , Train Loss : 0.11687623709440231 , Test Loss : 0.12446065247058868\n",
      "epoch 659/1000 , Train Loss : 0.11677704751491547 , Test Loss : 0.12437307089567184\n",
      "epoch 660/1000 , Train Loss : 0.11667808890342712 , Test Loss : 0.1242857500910759\n",
      "epoch 661/1000 , Train Loss : 0.11657939851284027 , Test Loss : 0.12419869750738144\n",
      "epoch 662/1000 , Train Loss : 0.11648094654083252 , Test Loss : 0.12411187589168549\n",
      "epoch 663/1000 , Train Loss : 0.11638277024030685 , Test Loss : 0.12402530759572983\n",
      "epoch 664/1000 , Train Loss : 0.1162848249077797 , Test Loss : 0.12393899261951447\n",
      "epoch 665/1000 , Train Loss : 0.11618714779615402 , Test Loss : 0.12385290116071701\n",
      "epoch 666/1000 , Train Loss : 0.11608970910310745 , Test Loss : 0.12376708537340164\n",
      "epoch 667/1000 , Train Loss : 0.11599252372980118 , Test Loss : 0.12368150055408478\n",
      "epoch 668/1000 , Train Loss : 0.115895576775074 , Test Loss : 0.12359616160392761\n",
      "epoch 669/1000 , Train Loss : 0.11579889804124832 , Test Loss : 0.12351107597351074\n",
      "epoch 670/1000 , Train Loss : 0.11570245027542114 , Test Loss : 0.12342622131109238\n",
      "epoch 671/1000 , Train Loss : 0.11560624837875366 , Test Loss : 0.12334159761667252\n",
      "epoch 672/1000 , Train Loss : 0.11551029235124588 , Test Loss : 0.12325723469257355\n",
      "epoch 673/1000 , Train Loss : 0.1154145747423172 , Test Loss : 0.12317311018705368\n",
      "epoch 674/1000 , Train Loss : 0.11531911045312881 , Test Loss : 0.12308920919895172\n",
      "epoch 675/1000 , Train Loss : 0.11522386968135834 , Test Loss : 0.12300556153059006\n",
      "epoch 676/1000 , Train Loss : 0.11512887477874756 , Test Loss : 0.1229221448302269\n",
      "epoch 677/1000 , Train Loss : 0.11503414064645767 , Test Loss : 0.12283894419670105\n",
      "epoch 678/1000 , Train Loss : 0.11493963748216629 , Test Loss : 0.1227560043334961\n",
      "epoch 679/1000 , Train Loss : 0.11484536528587341 , Test Loss : 0.12267328798770905\n",
      "epoch 680/1000 , Train Loss : 0.11475131660699844 , Test Loss : 0.1225908026099205\n",
      "epoch 681/1000 , Train Loss : 0.11465752124786377 , Test Loss : 0.12250854820013046\n",
      "epoch 682/1000 , Train Loss : 0.1145639419555664 , Test Loss : 0.12242653220891953\n",
      "epoch 683/1000 , Train Loss : 0.11447060108184814 , Test Loss : 0.1223447397351265\n",
      "epoch 684/1000 , Train Loss : 0.11437750607728958 , Test Loss : 0.12226317077875137\n",
      "epoch 685/1000 , Train Loss : 0.11428463459014893 , Test Loss : 0.12218182533979416\n",
      "epoch 686/1000 , Train Loss : 0.11419200897216797 , Test Loss : 0.12210071086883545\n",
      "epoch 687/1000 , Train Loss : 0.11409959942102432 , Test Loss : 0.12201983481645584\n",
      "epoch 688/1000 , Train Loss : 0.11400740593671799 , Test Loss : 0.12193914502859116\n",
      "epoch 689/1000 , Train Loss : 0.11391546577215195 , Test Loss : 0.12185871601104736\n",
      "epoch 690/1000 , Train Loss : 0.11382373422384262 , Test Loss : 0.12177850306034088\n",
      "epoch 691/1000 , Train Loss : 0.1137322336435318 , Test Loss : 0.12169851362705231\n",
      "epoch 692/1000 , Train Loss : 0.11364095658063889 , Test Loss : 0.12161872535943985\n",
      "epoch 693/1000 , Train Loss : 0.11354990303516388 , Test Loss : 0.1215391680598259\n",
      "epoch 694/1000 , Train Loss : 0.11345908790826797 , Test Loss : 0.12145985662937164\n",
      "epoch 695/1000 , Train Loss : 0.11336848884820938 , Test Loss : 0.12138072401285172\n",
      "epoch 696/1000 , Train Loss : 0.1132781058549881 , Test Loss : 0.12130182236433029\n",
      "epoch 697/1000 , Train Loss : 0.11318794637918472 , Test Loss : 0.12122315168380737\n",
      "epoch 698/1000 , Train Loss : 0.11309800297021866 , Test Loss : 0.12114468216896057\n",
      "epoch 699/1000 , Train Loss : 0.1130082905292511 , Test Loss : 0.12106642872095108\n",
      "epoch 700/1000 , Train Loss : 0.11291877925395966 , Test Loss : 0.1209883764386177\n",
      "epoch 701/1000 , Train Loss : 0.11282949894666672 , Test Loss : 0.12091056257486343\n",
      "epoch 702/1000 , Train Loss : 0.1127404272556305 , Test Loss : 0.12083295732736588\n",
      "epoch 703/1000 , Train Loss : 0.11265158653259277 , Test Loss : 0.12075554579496384\n",
      "epoch 704/1000 , Train Loss : 0.11256295442581177 , Test Loss : 0.1206783652305603\n",
      "epoch 705/1000 , Train Loss : 0.11247453838586807 , Test Loss : 0.1206013560295105\n",
      "epoch 706/1000 , Train Loss : 0.1123863235116005 , Test Loss : 0.12052459269762039\n",
      "epoch 707/1000 , Train Loss : 0.11229832470417023 , Test Loss : 0.1204480305314064\n",
      "epoch 708/1000 , Train Loss : 0.11221056431531906 , Test Loss : 0.12037166953086853\n",
      "epoch 709/1000 , Train Loss : 0.11212297528982162 , Test Loss : 0.12029549479484558\n",
      "epoch 710/1000 , Train Loss : 0.1120356097817421 , Test Loss : 0.12021955102682114\n",
      "epoch 711/1000 , Train Loss : 0.11194846779108047 , Test Loss : 0.120143823325634\n",
      "epoch 712/1000 , Train Loss : 0.11186154186725616 , Test Loss : 0.12006828188896179\n",
      "epoch 713/1000 , Train Loss : 0.11177480220794678 , Test Loss : 0.1199929267168045\n",
      "epoch 714/1000 , Train Loss : 0.1116882786154747 , Test Loss : 0.11991780251264572\n",
      "epoch 715/1000 , Train Loss : 0.11160194873809814 , Test Loss : 0.11984287947416306\n",
      "epoch 716/1000 , Train Loss : 0.1115158423781395 , Test Loss : 0.11976814270019531\n",
      "epoch 717/1000 , Train Loss : 0.11142993718385696 , Test Loss : 0.11969360709190369\n",
      "epoch 718/1000 , Train Loss : 0.11134422570466995 , Test Loss : 0.11961926519870758\n",
      "epoch 719/1000 , Train Loss : 0.11125873029232025 , Test Loss : 0.11954513192176819\n",
      "epoch 720/1000 , Train Loss : 0.11117342859506607 , Test Loss : 0.11947118490934372\n",
      "epoch 721/1000 , Train Loss : 0.11108832061290741 , Test Loss : 0.11939744651317596\n",
      "epoch 722/1000 , Train Loss : 0.11100342869758606 , Test Loss : 0.11932390183210373\n",
      "epoch 723/1000 , Train Loss : 0.11091874539852142 , Test Loss : 0.11925053596496582\n",
      "epoch 724/1000 , Train Loss : 0.11083424836397171 , Test Loss : 0.11917738616466522\n",
      "epoch 725/1000 , Train Loss : 0.11074995249509811 , Test Loss : 0.11910441517829895\n",
      "epoch 726/1000 , Train Loss : 0.11066585034132004 , Test Loss : 0.11903165280818939\n",
      "epoch 727/1000 , Train Loss : 0.11058195680379868 , Test Loss : 0.11895907670259476\n",
      "epoch 728/1000 , Train Loss : 0.11049826443195343 , Test Loss : 0.11888667941093445\n",
      "epoch 729/1000 , Train Loss : 0.11041474342346191 , Test Loss : 0.11881448328495026\n",
      "epoch 730/1000 , Train Loss : 0.11033143848180771 , Test Loss : 0.1187424585223198\n",
      "epoch 731/1000 , Train Loss : 0.11024831235408783 , Test Loss : 0.11867064982652664\n",
      "epoch 732/1000 , Train Loss : 0.11016538739204407 , Test Loss : 0.11859901994466782\n",
      "epoch 733/1000 , Train Loss : 0.11008264124393463 , Test Loss : 0.11852757632732391\n",
      "epoch 734/1000 , Train Loss : 0.1100001186132431 , Test Loss : 0.11845631897449493\n",
      "epoch 735/1000 , Train Loss : 0.10991775989532471 , Test Loss : 0.11838524788618088\n",
      "epoch 736/1000 , Train Loss : 0.10983561724424362 , Test Loss : 0.11831435561180115\n",
      "epoch 737/1000 , Train Loss : 0.10975366085767746 , Test Loss : 0.11824365705251694\n",
      "epoch 738/1000 , Train Loss : 0.10967187583446503 , Test Loss : 0.11817314475774765\n",
      "epoch 739/1000 , Train Loss : 0.10959029197692871 , Test Loss : 0.1181027963757515\n",
      "epoch 740/1000 , Train Loss : 0.10950890183448792 , Test Loss : 0.11803265661001205\n",
      "epoch 741/1000 , Train Loss : 0.10942768305540085 , Test Loss : 0.11796267330646515\n",
      "epoch 742/1000 , Train Loss : 0.1093466505408287 , Test Loss : 0.11789291352033615\n",
      "epoch 743/1000 , Train Loss : 0.10926581919193268 , Test Loss : 0.1178232952952385\n",
      "epoch 744/1000 , Train Loss : 0.10918517410755157 , Test Loss : 0.11775387823581696\n",
      "epoch 745/1000 , Train Loss : 0.1091047003865242 , Test Loss : 0.11768460273742676\n",
      "epoch 746/1000 , Train Loss : 0.10902441293001175 , Test Loss : 0.11761553585529327\n",
      "epoch 747/1000 , Train Loss : 0.10894432663917542 , Test Loss : 0.11754664778709412\n",
      "epoch 748/1000 , Train Loss : 0.10886440426111221 , Test Loss : 0.11747793108224869\n",
      "epoch 749/1000 , Train Loss : 0.10878466814756393 , Test Loss : 0.11740938574075699\n",
      "epoch 750/1000 , Train Loss : 0.10870513319969177 , Test Loss : 0.11734102666378021\n",
      "epoch 751/1000 , Train Loss : 0.10862573981285095 , Test Loss : 0.11727284640073776\n",
      "epoch 752/1000 , Train Loss : 0.10854655504226685 , Test Loss : 0.11720481514930725\n",
      "epoch 753/1000 , Train Loss : 0.10846754908561707 , Test Loss : 0.11713698506355286\n",
      "epoch 754/1000 , Train Loss : 0.10838871449232101 , Test Loss : 0.1170692965388298\n",
      "epoch 755/1000 , Train Loss : 0.10831006616353989 , Test Loss : 0.11700181663036346\n",
      "epoch 756/1000 , Train Loss : 0.10823158174753189 , Test Loss : 0.11693447828292847\n",
      "epoch 757/1000 , Train Loss : 0.10815329104661942 , Test Loss : 0.11686733365058899\n",
      "epoch 758/1000 , Train Loss : 0.10807516425848007 , Test Loss : 0.11680034548044205\n",
      "epoch 759/1000 , Train Loss : 0.10799721628427505 , Test Loss : 0.11673352122306824\n",
      "epoch 760/1000 , Train Loss : 0.10791943967342377 , Test Loss : 0.11666689068078995\n",
      "epoch 761/1000 , Train Loss : 0.1078418418765068 , Test Loss : 0.1166004091501236\n",
      "epoch 762/1000 , Train Loss : 0.10776443034410477 , Test Loss : 0.11653411388397217\n",
      "epoch 763/1000 , Train Loss : 0.10768716782331467 , Test Loss : 0.11646797508001328\n",
      "epoch 764/1000 , Train Loss : 0.10761009901762009 , Test Loss : 0.11640199273824692\n",
      "epoch 765/1000 , Train Loss : 0.10753318667411804 , Test Loss : 0.11633617430925369\n",
      "epoch 766/1000 , Train Loss : 0.10745644569396973 , Test Loss : 0.11627054959535599\n",
      "epoch 767/1000 , Train Loss : 0.10737987607717514 , Test Loss : 0.11620506644248962\n",
      "epoch 768/1000 , Train Loss : 0.10730350017547607 , Test Loss : 0.11613974720239639\n",
      "epoch 769/1000 , Train Loss : 0.10722728073596954 , Test Loss : 0.11607461422681808\n",
      "epoch 770/1000 , Train Loss : 0.10715121775865555 , Test Loss : 0.11600961536169052\n",
      "epoch 771/1000 , Train Loss : 0.10707533359527588 , Test Loss : 0.11594479531049728\n",
      "epoch 772/1000 , Train Loss : 0.10699961334466934 , Test Loss : 0.11588013172149658\n",
      "epoch 773/1000 , Train Loss : 0.10692407190799713 , Test Loss : 0.11581561714410782\n",
      "epoch 774/1000 , Train Loss : 0.10684868693351746 , Test Loss : 0.11575128138065338\n",
      "epoch 775/1000 , Train Loss : 0.10677347332239151 , Test Loss : 0.11568710207939148\n",
      "epoch 776/1000 , Train Loss : 0.1066984161734581 , Test Loss : 0.11562307178974152\n",
      "epoch 777/1000 , Train Loss : 0.10662352293729782 , Test Loss : 0.11555921286344528\n",
      "epoch 778/1000 , Train Loss : 0.10654880106449127 , Test Loss : 0.11549551039934158\n",
      "epoch 779/1000 , Train Loss : 0.10647425055503845 , Test Loss : 0.11543195694684982\n",
      "epoch 780/1000 , Train Loss : 0.10639984905719757 , Test Loss : 0.11536857485771179\n",
      "epoch 781/1000 , Train Loss : 0.10632563382387161 , Test Loss : 0.1153053343296051\n",
      "epoch 782/1000 , Train Loss : 0.1062515527009964 , Test Loss : 0.11524225771427155\n",
      "epoch 783/1000 , Train Loss : 0.10617764294147491 , Test Loss : 0.11517933011054993\n",
      "epoch 784/1000 , Train Loss : 0.10610390454530716 , Test Loss : 0.11511657387018204\n",
      "epoch 785/1000 , Train Loss : 0.10603030771017075 , Test Loss : 0.11505395919084549\n",
      "epoch 786/1000 , Train Loss : 0.10595688968896866 , Test Loss : 0.11499149352312088\n",
      "epoch 787/1000 , Train Loss : 0.10588361322879791 , Test Loss : 0.11492919921875\n",
      "epoch 788/1000 , Train Loss : 0.10581051558256149 , Test Loss : 0.11486703902482986\n",
      "epoch 789/1000 , Train Loss : 0.10573756694793701 , Test Loss : 0.11480503529310226\n",
      "epoch 790/1000 , Train Loss : 0.10566477477550507 , Test Loss : 0.1147431954741478\n",
      "epoch 791/1000 , Train Loss : 0.10559213161468506 , Test Loss : 0.11468148231506348\n",
      "epoch 792/1000 , Train Loss : 0.10551965981721878 , Test Loss : 0.11461994051933289\n",
      "epoch 793/1000 , Train Loss : 0.10544732958078384 , Test Loss : 0.11455852538347244\n",
      "epoch 794/1000 , Train Loss : 0.10537517070770264 , Test Loss : 0.11449728906154633\n",
      "epoch 795/1000 , Train Loss : 0.10530315339565277 , Test Loss : 0.11443618685007095\n",
      "epoch 796/1000 , Train Loss : 0.10523130744695663 , Test Loss : 0.11437523365020752\n",
      "epoch 797/1000 , Train Loss : 0.10515958070755005 , Test Loss : 0.11431442946195602\n",
      "epoch 798/1000 , Train Loss : 0.10508805513381958 , Test Loss : 0.11425375938415527\n",
      "epoch 799/1000 , Train Loss : 0.10501667112112045 , Test Loss : 0.11419326066970825\n",
      "epoch 800/1000 , Train Loss : 0.10494541376829147 , Test Loss : 0.11413289606571198\n",
      "epoch 801/1000 , Train Loss : 0.10487432032823563 , Test Loss : 0.11407268047332764\n",
      "epoch 802/1000 , Train Loss : 0.10480338335037231 , Test Loss : 0.11401258409023285\n",
      "epoch 803/1000 , Train Loss : 0.10473257303237915 , Test Loss : 0.11395267397165298\n",
      "epoch 804/1000 , Train Loss : 0.1046619638800621 , Test Loss : 0.11389287561178207\n",
      "epoch 805/1000 , Train Loss : 0.10459146648645401 , Test Loss : 0.1138332262635231\n",
      "epoch 806/1000 , Train Loss : 0.10452112555503845 , Test Loss : 0.11377373337745667\n",
      "epoch 807/1000 , Train Loss : 0.10445094108581543 , Test Loss : 0.11371438205242157\n",
      "epoch 808/1000 , Train Loss : 0.10438089817762375 , Test Loss : 0.11365515738725662\n",
      "epoch 809/1000 , Train Loss : 0.1043110117316246 , Test Loss : 0.11359609663486481\n",
      "epoch 810/1000 , Train Loss : 0.1042412668466568 , Test Loss : 0.11353715509176254\n",
      "epoch 811/1000 , Train Loss : 0.10417164862155914 , Test Loss : 0.11347837746143341\n",
      "epoch 812/1000 , Train Loss : 0.10410220175981522 , Test Loss : 0.11341970413923264\n",
      "epoch 813/1000 , Train Loss : 0.10403289645910263 , Test Loss : 0.11336119472980499\n",
      "epoch 814/1000 , Train Loss : 0.10396374017000198 , Test Loss : 0.11330283433198929\n",
      "epoch 815/1000 , Train Loss : 0.10389471799135208 , Test Loss : 0.11324459314346313\n",
      "epoch 816/1000 , Train Loss : 0.10382584482431412 , Test Loss : 0.11318648606538773\n",
      "epoch 817/1000 , Train Loss : 0.10375712066888809 , Test Loss : 0.11312854290008545\n",
      "epoch 818/1000 , Train Loss : 0.10368853807449341 , Test Loss : 0.11307071894407272\n",
      "epoch 819/1000 , Train Loss : 0.10362009704113007 , Test Loss : 0.11301303654909134\n",
      "epoch 820/1000 , Train Loss : 0.10355179756879807 , Test Loss : 0.1129554808139801\n",
      "epoch 821/1000 , Train Loss : 0.1034836396574974 , Test Loss : 0.1128980740904808\n",
      "epoch 822/1000 , Train Loss : 0.10341563820838928 , Test Loss : 0.11284080147743225\n",
      "epoch 823/1000 , Train Loss : 0.10334774851799011 , Test Loss : 0.11278364807367325\n",
      "epoch 824/1000 , Train Loss : 0.10328002274036407 , Test Loss : 0.11272665113210678\n",
      "epoch 825/1000 , Train Loss : 0.10321243107318878 , Test Loss : 0.11266978085041046\n",
      "epoch 826/1000 , Train Loss : 0.10314497351646423 , Test Loss : 0.11261303722858429\n",
      "epoch 827/1000 , Train Loss : 0.10307765752077103 , Test Loss : 0.11255642771720886\n",
      "epoch 828/1000 , Train Loss : 0.10301049798727036 , Test Loss : 0.11249995976686478\n",
      "epoch 829/1000 , Train Loss : 0.10294345021247864 , Test Loss : 0.11244361847639084\n",
      "epoch 830/1000 , Train Loss : 0.10287655889987946 , Test Loss : 0.11238740384578705\n",
      "epoch 831/1000 , Train Loss : 0.10280980169773102 , Test Loss : 0.11233130842447281\n",
      "epoch 832/1000 , Train Loss : 0.10274317860603333 , Test Loss : 0.11227536201477051\n",
      "epoch 833/1000 , Train Loss : 0.10267668962478638 , Test Loss : 0.11221954226493835\n",
      "epoch 834/1000 , Train Loss : 0.10261033475399017 , Test Loss : 0.11216385662555695\n",
      "epoch 835/1000 , Train Loss : 0.10254412144422531 , Test Loss : 0.11210829764604568\n",
      "epoch 836/1000 , Train Loss : 0.10247805714607239 , Test Loss : 0.11205287277698517\n",
      "epoch 837/1000 , Train Loss : 0.10241210460662842 , Test Loss : 0.1119975671172142\n",
      "epoch 838/1000 , Train Loss : 0.10234629362821579 , Test Loss : 0.11194239556789398\n",
      "epoch 839/1000 , Train Loss : 0.1022806242108345 , Test Loss : 0.11188734322786331\n",
      "epoch 840/1000 , Train Loss : 0.10221507400274277 , Test Loss : 0.11183242499828339\n",
      "epoch 841/1000 , Train Loss : 0.10214968025684357 , Test Loss : 0.11177763342857361\n",
      "epoch 842/1000 , Train Loss : 0.10208440572023392 , Test Loss : 0.11172297596931458\n",
      "epoch 843/1000 , Train Loss : 0.10201926529407501 , Test Loss : 0.1116684153676033\n",
      "epoch 844/1000 , Train Loss : 0.10195425152778625 , Test Loss : 0.11161399632692337\n",
      "epoch 845/1000 , Train Loss : 0.10188936442136765 , Test Loss : 0.11155971139669418\n",
      "epoch 846/1000 , Train Loss : 0.10182462632656097 , Test Loss : 0.11150555312633514\n",
      "epoch 847/1000 , Train Loss : 0.10176001489162445 , Test Loss : 0.11145151406526566\n",
      "epoch 848/1000 , Train Loss : 0.10169552266597748 , Test Loss : 0.11139759421348572\n",
      "epoch 849/1000 , Train Loss : 0.10163115710020065 , Test Loss : 0.11134380102157593\n",
      "epoch 850/1000 , Train Loss : 0.10156693309545517 , Test Loss : 0.11129011958837509\n",
      "epoch 851/1000 , Train Loss : 0.10150282829999924 , Test Loss : 0.1112365797162056\n",
      "epoch 852/1000 , Train Loss : 0.10143885761499405 , Test Loss : 0.11118315905332565\n",
      "epoch 853/1000 , Train Loss : 0.10137501358985901 , Test Loss : 0.11112984269857407\n",
      "epoch 854/1000 , Train Loss : 0.10131131112575531 , Test Loss : 0.11107666045427322\n",
      "epoch 855/1000 , Train Loss : 0.10124772042036057 , Test Loss : 0.11102359741926193\n",
      "epoch 856/1000 , Train Loss : 0.10118427127599716 , Test Loss : 0.11097066104412079\n",
      "epoch 857/1000 , Train Loss : 0.10112094134092331 , Test Loss : 0.1109178364276886\n",
      "epoch 858/1000 , Train Loss : 0.10105773061513901 , Test Loss : 0.11086514592170715\n",
      "epoch 859/1000 , Train Loss : 0.10099463909864426 , Test Loss : 0.11081255972385406\n",
      "epoch 860/1000 , Train Loss : 0.10093167424201965 , Test Loss : 0.11076009273529053\n",
      "epoch 861/1000 , Train Loss : 0.10086885094642639 , Test Loss : 0.11070775985717773\n",
      "epoch 862/1000 , Train Loss : 0.10080614686012268 , Test Loss : 0.1106555238366127\n",
      "epoch 863/1000 , Train Loss : 0.10074356198310852 , Test Loss : 0.11060341447591782\n",
      "epoch 864/1000 , Train Loss : 0.10068109631538391 , Test Loss : 0.11055141687393188\n",
      "epoch 865/1000 , Train Loss : 0.10061875730752945 , Test Loss : 0.1104995533823967\n",
      "epoch 866/1000 , Train Loss : 0.10055655986070633 , Test Loss : 0.11044779419898987\n",
      "epoch 867/1000 , Train Loss : 0.10049446672201157 , Test Loss : 0.1103961393237114\n",
      "epoch 868/1000 , Train Loss : 0.10043250769376755 , Test Loss : 0.11034463346004486\n",
      "epoch 869/1000 , Train Loss : 0.10037066042423248 , Test Loss : 0.11029321700334549\n",
      "epoch 870/1000 , Train Loss : 0.10030893981456757 , Test Loss : 0.11024192720651627\n",
      "epoch 871/1000 , Train Loss : 0.1002473309636116 , Test Loss : 0.1101907417178154\n",
      "epoch 872/1000 , Train Loss : 0.10018584877252579 , Test Loss : 0.11013967543840408\n",
      "epoch 873/1000 , Train Loss : 0.10012448579072952 , Test Loss : 0.11008872836828232\n",
      "epoch 874/1000 , Train Loss : 0.10006324201822281 , Test Loss : 0.11003787815570831\n",
      "epoch 875/1000 , Train Loss : 0.10000211745500565 , Test Loss : 0.10998715460300446\n",
      "epoch 876/1000 , Train Loss : 0.09994112700223923 , Test Loss : 0.10993654280900955\n",
      "epoch 877/1000 , Train Loss : 0.09988023340702057 , Test Loss : 0.109886035323143\n",
      "epoch 878/1000 , Train Loss : 0.09981948882341385 , Test Loss : 0.1098356619477272\n",
      "epoch 879/1000 , Train Loss : 0.0997588261961937 , Test Loss : 0.10978538542985916\n",
      "epoch 880/1000 , Train Loss : 0.09969829767942429 , Test Loss : 0.10973522067070007\n",
      "epoch 881/1000 , Train Loss : 0.09963790327310562 , Test Loss : 0.10968516767024994\n",
      "epoch 882/1000 , Train Loss : 0.09957760572433472 , Test Loss : 0.10963522642850876\n",
      "epoch 883/1000 , Train Loss : 0.09951741993427277 , Test Loss : 0.10958540439605713\n",
      "epoch 884/1000 , Train Loss : 0.09945736080408096 , Test Loss : 0.10953568667173386\n",
      "epoch 885/1000 , Train Loss : 0.09939741343259811 , Test Loss : 0.10948606580495834\n",
      "epoch 886/1000 , Train Loss : 0.09933758527040482 , Test Loss : 0.10943657159805298\n",
      "epoch 887/1000 , Train Loss : 0.09927786141633987 , Test Loss : 0.10938717424869537\n",
      "epoch 888/1000 , Train Loss : 0.09921826422214508 , Test Loss : 0.10933788865804672\n",
      "epoch 889/1000 , Train Loss : 0.09915877878665924 , Test Loss : 0.10928871482610703\n",
      "epoch 890/1000 , Train Loss : 0.09909942001104355 , Test Loss : 0.10923964530229568\n",
      "epoch 891/1000 , Train Loss : 0.09904015064239502 , Test Loss : 0.1091906875371933\n",
      "epoch 892/1000 , Train Loss : 0.09898102283477783 , Test Loss : 0.10914183408021927\n",
      "epoch 893/1000 , Train Loss : 0.09892197698354721 , Test Loss : 0.1090930849313736\n",
      "epoch 894/1000 , Train Loss : 0.09886307269334793 , Test Loss : 0.10904444009065628\n",
      "epoch 895/1000 , Train Loss : 0.09880426526069641 , Test Loss : 0.10899592190980911\n",
      "epoch 896/1000 , Train Loss : 0.09874558448791504 , Test Loss : 0.10894748568534851\n",
      "epoch 897/1000 , Train Loss : 0.09868698567152023 , Test Loss : 0.10889915376901627\n",
      "epoch 898/1000 , Train Loss : 0.09862852096557617 , Test Loss : 0.10885094106197357\n",
      "epoch 899/1000 , Train Loss : 0.09857016801834106 , Test Loss : 0.10880283266305923\n",
      "epoch 900/1000 , Train Loss : 0.09851191937923431 , Test Loss : 0.10875482112169266\n",
      "epoch 901/1000 , Train Loss : 0.09845377504825592 , Test Loss : 0.10870692133903503\n",
      "epoch 902/1000 , Train Loss : 0.09839575737714767 , Test Loss : 0.10865912586450577\n",
      "epoch 903/1000 , Train Loss : 0.09833783656358719 , Test Loss : 0.10861141979694366\n",
      "epoch 904/1000 , Train Loss : 0.09828002750873566 , Test Loss : 0.10856384038925171\n",
      "epoch 905/1000 , Train Loss : 0.09822233766317368 , Test Loss : 0.10851635038852692\n",
      "epoch 906/1000 , Train Loss : 0.09816474467515945 , Test Loss : 0.10846897214651108\n",
      "epoch 907/1000 , Train Loss : 0.09810727089643478 , Test Loss : 0.1084216758608818\n",
      "epoch 908/1000 , Train Loss : 0.09804988652467728 , Test Loss : 0.10837449878454208\n",
      "epoch 909/1000 , Train Loss : 0.09799264371395111 , Test Loss : 0.10832741111516953\n",
      "epoch 910/1000 , Train Loss : 0.09793548285961151 , Test Loss : 0.10828044265508652\n",
      "epoch 911/1000 , Train Loss : 0.09787842631340027 , Test Loss : 0.10823355615139008\n",
      "epoch 912/1000 , Train Loss : 0.09782148152589798 , Test Loss : 0.10818677395582199\n",
      "epoch 913/1000 , Train Loss : 0.09776464104652405 , Test Loss : 0.10814011096954346\n",
      "epoch 914/1000 , Train Loss : 0.09770791977643967 , Test Loss : 0.1080935150384903\n",
      "epoch 915/1000 , Train Loss : 0.09765130281448364 , Test Loss : 0.10804704576730728\n",
      "epoch 916/1000 , Train Loss : 0.09759478271007538 , Test Loss : 0.10800065845251083\n",
      "epoch 917/1000 , Train Loss : 0.09753835946321487 , Test Loss : 0.10795438289642334\n",
      "epoch 918/1000 , Train Loss : 0.09748206287622452 , Test Loss : 0.1079082041978836\n",
      "epoch 919/1000 , Train Loss : 0.09742585569620132 , Test Loss : 0.10786212235689163\n",
      "epoch 920/1000 , Train Loss : 0.09736976027488708 , Test Loss : 0.10781615972518921\n",
      "epoch 921/1000 , Train Loss : 0.0973137766122818 , Test Loss : 0.10777027159929276\n",
      "epoch 922/1000 , Train Loss : 0.09725787490606308 , Test Loss : 0.10772448778152466\n",
      "epoch 923/1000 , Train Loss : 0.09720209240913391 , Test Loss : 0.10767878592014313\n",
      "epoch 924/1000 , Train Loss : 0.09714639186859131 , Test Loss : 0.10763320326805115\n",
      "epoch 925/1000 , Train Loss : 0.09709081798791885 , Test Loss : 0.10758769512176514\n",
      "epoch 926/1000 , Train Loss : 0.09703534096479416 , Test Loss : 0.10754229128360748\n",
      "epoch 927/1000 , Train Loss : 0.09697996824979782 , Test Loss : 0.10749699175357819\n",
      "epoch 928/1000 , Train Loss : 0.09692468494176865 , Test Loss : 0.10745179653167725\n",
      "epoch 929/1000 , Train Loss : 0.09686952084302902 , Test Loss : 0.10740667581558228\n",
      "epoch 930/1000 , Train Loss : 0.09681445360183716 , Test Loss : 0.10736165940761566\n",
      "epoch 931/1000 , Train Loss : 0.09675947576761246 , Test Loss : 0.10731673240661621\n",
      "epoch 932/1000 , Train Loss : 0.09670460969209671 , Test Loss : 0.10727191716432571\n",
      "epoch 933/1000 , Train Loss : 0.09664984792470932 , Test Loss : 0.10722718387842178\n",
      "epoch 934/1000 , Train Loss : 0.0965951681137085 , Test Loss : 0.10718254745006561\n",
      "epoch 935/1000 , Train Loss : 0.09654060751199722 , Test Loss : 0.1071380004286766\n",
      "epoch 936/1000 , Train Loss : 0.09648613631725311 , Test Loss : 0.10709355026483536\n",
      "epoch 937/1000 , Train Loss : 0.09643177688121796 , Test Loss : 0.10704918950796127\n",
      "epoch 938/1000 , Train Loss : 0.09637750685214996 , Test Loss : 0.10700494050979614\n",
      "epoch 939/1000 , Train Loss : 0.09632333368062973 , Test Loss : 0.10696075856685638\n",
      "epoch 940/1000 , Train Loss : 0.09626926481723785 , Test Loss : 0.10691668093204498\n",
      "epoch 941/1000 , Train Loss : 0.09621528536081314 , Test Loss : 0.10687268525362015\n",
      "epoch 942/1000 , Train Loss : 0.09616141021251678 , Test Loss : 0.10682881623506546\n",
      "epoch 943/1000 , Train Loss : 0.09610763192176819 , Test Loss : 0.10678500682115555\n",
      "epoch 944/1000 , Train Loss : 0.09605394303798676 , Test Loss : 0.1067412868142128\n",
      "epoch 945/1000 , Train Loss : 0.09600036591291428 , Test Loss : 0.10669767111539841\n",
      "epoch 946/1000 , Train Loss : 0.09594689309597015 , Test Loss : 0.10665413737297058\n",
      "epoch 947/1000 , Train Loss : 0.095893494784832 , Test Loss : 0.1066107228398323\n",
      "epoch 948/1000 , Train Loss : 0.09584018588066101 , Test Loss : 0.10656736046075821\n",
      "epoch 949/1000 , Train Loss : 0.09578699618577957 , Test Loss : 0.10652409493923187\n",
      "epoch 950/1000 , Train Loss : 0.0957338958978653 , Test Loss : 0.1064809262752533\n",
      "epoch 951/1000 , Train Loss : 0.09568087011575699 , Test Loss : 0.10643786936998367\n",
      "epoch 952/1000 , Train Loss : 0.09562797099351883 , Test Loss : 0.10639486461877823\n",
      "epoch 953/1000 , Train Loss : 0.09557514637708664 , Test Loss : 0.10635195672512054\n",
      "epoch 954/1000 , Train Loss : 0.0955224260687828 , Test Loss : 0.10630916059017181\n",
      "epoch 955/1000 , Train Loss : 0.09546980261802673 , Test Loss : 0.10626643151044846\n",
      "epoch 956/1000 , Train Loss : 0.09541727602481842 , Test Loss : 0.10622379928827286\n",
      "epoch 957/1000 , Train Loss : 0.09536483138799667 , Test Loss : 0.10618124902248383\n",
      "epoch 958/1000 , Train Loss : 0.09531248360872269 , Test Loss : 0.10613879561424255\n",
      "epoch 959/1000 , Train Loss : 0.09526022523641586 , Test Loss : 0.10609642416238785\n",
      "epoch 960/1000 , Train Loss : 0.0952080637216568 , Test Loss : 0.10605411976575851\n",
      "epoch 961/1000 , Train Loss : 0.0951559916138649 , Test Loss : 0.10601191222667694\n",
      "epoch 962/1000 , Train Loss : 0.09510402381420135 , Test Loss : 0.10596982389688492\n",
      "epoch 963/1000 , Train Loss : 0.09505213052034378 , Test Loss : 0.10592778772115707\n",
      "epoch 964/1000 , Train Loss : 0.09500034153461456 , Test Loss : 0.10588585585355759\n",
      "epoch 965/1000 , Train Loss : 0.0949486494064331 , Test Loss : 0.10584399849176407\n",
      "epoch 966/1000 , Train Loss : 0.09489704668521881 , Test Loss : 0.10580223798751831\n",
      "epoch 967/1000 , Train Loss : 0.09484553337097168 , Test Loss : 0.10576056689023972\n",
      "epoch 968/1000 , Train Loss : 0.09479410946369171 , Test Loss : 0.10571897774934769\n",
      "epoch 969/1000 , Train Loss : 0.0947427749633789 , Test Loss : 0.10567745566368103\n",
      "epoch 970/1000 , Train Loss : 0.09469152241945267 , Test Loss : 0.10563603043556213\n",
      "epoch 971/1000 , Train Loss : 0.09464036673307419 , Test Loss : 0.1055946797132492\n",
      "epoch 972/1000 , Train Loss : 0.09458930790424347 , Test Loss : 0.10555341839790344\n",
      "epoch 973/1000 , Train Loss : 0.09453833103179932 , Test Loss : 0.10551224648952484\n",
      "epoch 974/1000 , Train Loss : 0.09448745101690292 , Test Loss : 0.105471171438694\n",
      "epoch 975/1000 , Train Loss : 0.0944366529583931 , Test Loss : 0.10543015599250793\n",
      "epoch 976/1000 , Train Loss : 0.09438594430685043 , Test Loss : 0.10538922250270844\n",
      "epoch 977/1000 , Train Loss : 0.09433532506227493 , Test Loss : 0.10534840822219849\n",
      "epoch 978/1000 , Train Loss : 0.09428480267524719 , Test Loss : 0.10530763119459152\n",
      "epoch 979/1000 , Train Loss : 0.09423436224460602 , Test Loss : 0.10526695102453232\n",
      "epoch 980/1000 , Train Loss : 0.09418399631977081 , Test Loss : 0.10522636771202087\n",
      "epoch 981/1000 , Train Loss : 0.09413374215364456 , Test Loss : 0.1051858440041542\n",
      "epoch 982/1000 , Train Loss : 0.09408354759216309 , Test Loss : 0.10514543205499649\n",
      "epoch 983/1000 , Train Loss : 0.09403345733880997 , Test Loss : 0.10510509461164474\n",
      "epoch 984/1000 , Train Loss : 0.09398345649242401 , Test Loss : 0.10506480187177658\n",
      "epoch 985/1000 , Train Loss : 0.09393355250358582 , Test Loss : 0.10502463579177856\n",
      "epoch 986/1000 , Train Loss : 0.09388372302055359 , Test Loss : 0.10498452186584473\n",
      "epoch 987/1000 , Train Loss : 0.09383397549390793 , Test Loss : 0.10494450479745865\n",
      "epoch 988/1000 , Train Loss : 0.09378430247306824 , Test Loss : 0.10490454733371735\n",
      "epoch 989/1000 , Train Loss : 0.0937347412109375 , Test Loss : 0.1048646867275238\n",
      "epoch 990/1000 , Train Loss : 0.09368524700403214 , Test Loss : 0.10482491552829742\n",
      "epoch 991/1000 , Train Loss : 0.09363586455583572 , Test Loss : 0.10478520393371582\n",
      "epoch 992/1000 , Train Loss : 0.09358653426170349 , Test Loss : 0.10474558174610138\n",
      "epoch 993/1000 , Train Loss : 0.09353730082511902 , Test Loss : 0.1047060415148735\n",
      "epoch 994/1000 , Train Loss : 0.0934881642460823 , Test Loss : 0.1046665757894516\n",
      "epoch 995/1000 , Train Loss : 0.09343909472227097 , Test Loss : 0.10462717711925507\n",
      "epoch 996/1000 , Train Loss : 0.09339011460542679 , Test Loss : 0.1045878678560257\n",
      "epoch 997/1000 , Train Loss : 0.09334123134613037 , Test Loss : 0.10454864799976349\n",
      "epoch 998/1000 , Train Loss : 0.09329240769147873 , Test Loss : 0.10450948774814606\n",
      "epoch 999/1000 , Train Loss : 0.09324368834495544 , Test Loss : 0.10447042435407639\n",
      "epoch 1000/1000 , Train Loss : 0.09319503605365753 , Test Loss : 0.10443142801523209\n"
     ]
    }
   ],
   "source": [
    "n_epoches = 1000\n",
    "\n",
    "TrainLoss = []\n",
    "TestLoss = []\n",
    "\n",
    "for epoch in range(n_epoches):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    prediction = model(Xtrain)\n",
    "    loss = criterion(prediction, Ytrain)\n",
    "    TrainLoss.append(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    prediction_test = model(Xtest)\n",
    "    loss_test = criterion(prediction_test, Ytest)\n",
    "    TestLoss.append(loss_test.item())\n",
    "    \n",
    "    print(\"epoch {}/{} , Train Loss : {} , Test Loss : {}\".format(epoch+1, n_epoches, loss.item(), loss_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a3f978-c844-4706-b408-645ebfae6c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TrainLoss, label='Train Loss')\n",
    "plt.plot(TestLoss, label='Test Loss')\n",
    "plt.xlabel('Epoches')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c433fa76-8c6c-497d-bbd0-74dbcf92597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc : 0.9859484777517564 , Test Acc : 0.9647887323943662\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy\n",
    "\n",
    "with torch.no_grad():\n",
    "    Ptrain = model(Xtrain).numpy()\n",
    "    Ptest  = model(Xtest).numpy()\n",
    "    \n",
    "    Ptrain = Ptrain > 0.5\n",
    "    Ptest  = Ptest > 0.5\n",
    "    \n",
    "    train_acc = np.mean(Ptrain == Ytrain.numpy())\n",
    "    test_acc = np.mean(Ptest == Ytest.numpy())\n",
    "    print(\"Train Accuracy : {} , Test Accuracy : {}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "005cd017-b8a9-46f9-8831-27189cfbcb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, 'cancer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d9048a-2498-4d98-a53a-99f9c047ebfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model\n",
    "\n",
    "model_load = nn.Sequential(\n",
    "                nn.Linear(Xtrain.shape[1], 1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "model_load.load_state_dict(\n",
    "                    torch.load('cancer.pt')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b253c19-9494-42aa-8d07-465079701cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading .....\n",
      "Train Accuracy : 0.9859484777517564 , Test Accuracy : 0.9647887323943662\n"
     ]
    }
   ],
   "source": [
    "# Check Model Accuracy Again\n",
    "\n",
    "with torch.no_grad():\n",
    "    Ptrain = model_load(Xtrain).numpy()\n",
    "    Ptest  = model_load(Xtest).numpy()\n",
    "    \n",
    "    Ptrain = Ptrain > 0.5\n",
    "    Ptest  = Ptest > 0.5\n",
    "    \n",
    "    train_acc = np.mean(Ptrain == Ytrain.numpy())\n",
    "    test_acc = np.mean(Ptest == Ytest.numpy())\n",
    "    print(\"Model Loading .....\")\n",
    "    print(\"Train Accuracy : {} , Test Accuracy : {}\".format(train_acc, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
